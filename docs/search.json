[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regresión No Lineal",
    "section": "",
    "text": "Presentación\nProyecto de Estadística de Regresión No Lineal.\n\n\n\n\nBates, Douglas M., y Donald G. Watts. 1988. Nonlinear regression analysis and its applications. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nGaray, Aldo M., Víctor H. Lachos, Filidor V. Labra, y Edwin M. M. Ortega and. 2014. «Statistical diagnostics for nonlinear regression models based on scale mixtures of skew-normal distributions». Journal of Statistical Computation and Simulation 84 (8): 1761-78. https://doi.org/10.1080/00949655.2013.766188.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, y G. Geoffrey Vining. 2012. Introduction to linear regression analysis. 5.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nRitz, Christian, y Jens C. Streibig. 2008. Nonlinear Regression with R. Use R! New York: Springer. https://doi.org/10.1007/978-0-387-09616-2.\n\n\nRuckstuhl, Andreas. 2010. «Introduction to Nonlinear Regression». IDP Institut für Datenanalyse und Prozessdesign, ZHAW Zürcher Hochschule für Angewandte Wissenschaften.\n\n\nSchabenberger, Oliver, y Francis J. Pierce. 2002. Contemporary Statistical Models for the Plant and Soil Sciences. Taylor & Francis, CRC Press.\n\n\nSeber, G. A. F, y Wild C. J. 2003. Nonlinear regression. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: John Wiley & Sons.",
    "crumbs": [
      "Presentación"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html",
    "href": "Caps/01-Introduccion.html",
    "title": "1  Regresión no lineal",
    "section": "",
    "text": "1.1 Introducción\nUna tarea importante en estadística es encontrar las relaciones, si existen, en un conjunto de variables cuando al menos una es aleatoria, estando sujeta a fluctuaciones aleatorias y posiblemente a errores de medición. En los problemas de regresión, típicamente una de las variables, a menudo llamada variable de respuesta o dependiente, es de particular interés y se denota por \\({y}\\). Las otras variables \\({x}_{1}, {x}_{2}, \\ldots, {x}_{k}\\), usualmente llamadas variables explicativas, regresoras o independientes, se utilizan principalmente para predecir o explicar el comportamiento de \\({y}\\). Si los gráficos de los datos sugieren alguna relación entre \\({y}\\) y las \\({x}_{i}\\), entonces esperaríamos expresar esta relación mediante alguna función \\(f\\), es decir, \\[\n    {y} \\approx f({x}_{1}, {x}_{2}, \\ldots, {x}_{k}).\n\\qquad(1.1)\\] Usando \\(f\\), podríamos entonces predecir \\({y}\\) para un conjunto dado de \\({x}\\). Por ejemplo, \\({y}\\) podría ser el precio de un automóvil usado de cierta marca, \\({x}_{1}\\) el número de dueños anteriores, \\({x}_{2}\\) la edad del automóvil y \\({x}_{3}\\) el kilometraje. Como en la Ecuación 1.1, las relaciones nunca pueden ser exactas, ya que los datos frecuentemente contendrán fluctuaciones no explicadas o ruido, y usualmente habrá algún grado de error de medición.\nLas variables explicativas pueden ser aleatorias o fijas (por ejemplo, controladas). Consideremos un experimento realizado para medir el rendimiento (\\({y}\\)) de trigo en varios niveles específicos de densidad de siembra (\\({x}_{1}\\)) y aplicación de fertilizante (\\({x}_{2}\\)). Entonces, tanto \\({x}_{1}\\) como \\({x}_{2}\\) son fijos. Si, al momento de la siembra, también se midió el pH del suelo (\\({x}_{3}\\)) en cada parcela, entonces \\({x}_{3}\\) sería aleatorio.\nA veces, la forma matemática apropiada para la relación en la Ecuación 1.1 es conocida, excepto por algunas constantes o coeficientes desconocidos (llamados parámetros), donde la relación está determinada por un proceso físico subyacente conocido o gobernado por leyes científicas aceptadas. Matemáticamente, la relación en la Ecuación 1.1 puede escribirse como \\[\n    {y} \\approx f({x}_{1}, {x}_{2}, \\ldots, {x}_{k}; \\boldsymbol{\\theta}),\n\\qquad(1.2)\\] donde \\(f\\) es completamente conocida excepto por el vector de parámetros \\(\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{p})^\\top\\), que necesita ser estimado. Por ejemplo, \\[\n    {y} \\approx \\theta_1 e^{\\theta_2 \\boldsymbol{x}},\n\\] y \\(\\boldsymbol\\theta = (\\theta_1, \\theta_2)^\\top\\). Dado que estos parámetros a menudo tienen interpretaciones físicas, uno de los objetivos principales de la investigación es estimar los parámetros con la mayor precisión posible. Frecuentemente, la relación en la Ecuación 1.2 es sugerida tentativamente por investigaciones teóricas. Otro de los objetivos es probar el ajuste de los datos al modelo.\nSin embargo, en gran parte de la práctica estadística, particularmente en las ciencias biológicas, en oposición a las ciencias físicas, los procesos subyacentes son generalmente complejos y no son bien comprendidos. Esto significa que tenemos poca o ninguna idea sobre la forma de la relación, y nuestro objetivo es simplemente encontrar alguna función \\(f\\) para la cual la Ecuación 1.2 se cumpla lo más cerca posible. Generalmente se asume que la relación “verdadera” pertenece a una familia paramétrica de la forma Ecuación 1.2, de modo que encontrar un modelo a menudo se reduce a estimar \\(\\boldsymbol{\\theta}\\). En este caso, es importante utilizar una familia que sea lo suficientemente amplia y flexible para aproximar una variedad lo bastante extensa de formas funcionales. Si varios modelos ajustan los datos igualmente bien, usualmente elegiríamos el más simple (principio de parsimonia). Por ejemplo, podría usarse para predecir \\({y}\\), o incluso controlar \\({y}\\), ajustando las variables \\(\\boldsymbol{x}\\). En algunas situaciones, también podemos usar el modelo para calibración (predicción inversa), donde predecimos \\(\\boldsymbol{x}\\) para un nuevo valor de \\({y}\\).\nEn el análisis de regresión lineal, como vimos durante el curso, se utilizan modelos con errores aditivos. Así, tenemos \\[\n    \\boldsymbol{y} = \\beta_{0} + \\beta_{1}\\boldsymbol{x}_{1} + \\cdots + \\beta_{p-1}\\boldsymbol{x}_{p-1} + \\boldsymbol\\varepsilon,\n\\] donde \\(\\boldsymbol\\varepsilon \\sim N_n(\\boldsymbol 0, \\sigma^2 I_n)\\). Aquí, \\(\\boldsymbol\\beta = (\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p-1})^\\top\\), y las \\(\\boldsymbol{x}_{i}\\) pueden incluir transformaciones como cuadrados, productos cruzados, potencias superiores e incluso transformaciones (por ejemplo, logaritmos) de las mediciones originales. El requisito importante es que la expresión debe ser lineal en los parámetros. Por ejemplo, \\[\n    \\boldsymbol{y} = \\beta_{0} + \\beta_{1}\\boldsymbol{x}_{1} + \\beta_{2}\\boldsymbol{x}_{2} + \\beta_{3}\\boldsymbol{x}_{1}^{2} + \\beta_{4}\\boldsymbol{x}_{2}^{2} + \\beta_{5}\\boldsymbol{x}_{1}\\boldsymbol{x}_{2} + \\boldsymbol\\varepsilon\n\\] y \\[\n    \\boldsymbol{y} = \\beta_{0} + \\beta_{1}\\sin \\boldsymbol{x}_{1} + \\beta_{2}\\sin \\boldsymbol{x}_{2} + \\boldsymbol\\varepsilon\n\\] son ambos modelos lineales. De estos ejemplos vemos que la familia de modelos lineales es muy flexible, por lo que a menudo se utiliza en ausencia de un modelo teórico para \\(f\\).\nSin embargo, a menudo se cuenta con una expresión matemática que relaciona las variables de respuesta con las variables predictoras, y estos modelos suelen ser no lineales en sus parámetros. Incluso cuando una aproximación lineal funciona bien, un modelo no lineal aún puede usarse para mantener una interpretación clara de los parámetros. En tales casos, es necesario ampliar las técnicas de regresión lineal, lo que introduce una complejidad considerable. Por ejemplo \\[\n    {y} = \\theta_{0} + \\theta_{1}e^{\\theta_{2}{x}} + \\varepsilon\n\\] es un modelo no lineal, ya que es no lineal en \\(\\theta_{2}\\).\nCualquier modelo que no sea lineal en los parámetros desconocidos es un modelo de regresión no lineal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regresión no lineal</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#modelos-no-lineales-comunes",
    "href": "Caps/01-Introduccion.html#modelos-no-lineales-comunes",
    "title": "1  Regresión no lineal",
    "section": "1.2 Modelos no lineales comunes",
    "text": "1.2 Modelos no lineales comunes\nA continuación presentamos varios ejemplos comunes de modelos no lineales, que nos permitirán entender el tipo de problemas que estaremos abordando, antes de entrar a los detalles de la regresión no lineal.\n\nEjemplo 1.1 (Modelo exponencial de crecimiento.) Malthus, economista británico afirmaba que la tasa de cambio de una población con respecto al tiempo era proporcional a la población. Si \\(t\\) denota el tiempo, \\(y\\) el tamaño de la población y \\(k\\) la tasa de crecimiento, el modelo propuesto es \\[\n    \\frac{dy}{dt} = ky.\n\\] Si suponemos que la población inicial \\(y(0) = \\alpha\\), se tiene que la solución es \\[\n    y = \\alpha e^{kt}.\n\\qquad(1.3)\\]\n\n\nEjemplo 1.2 (Modelo logístico de crecimiento.) Verhulst consideró una variante en la que el ambiente no puede soportar mas que cierta población máxima \\(M\\), así que la población crece de manera acelerada y a partir de cierto punto el tamaño de la población se ralentiza. Si \\(t\\) denota el tiempo, \\(y\\) el tamaño de la población, \\(k\\) la tasa de crecimiento se tiene que \\[\\begin{align*}\n    \\frac{d y}{d t} = k y \\left(1 - \\frac{y}{M}\\right).\n\\end{align*}\\] donde \\(y(t) \\to M\\) cuando \\(t \\to \\infty\\). Suponiendo que la población inicial es \\(\\alpha\\), esto es \\(y(0) = \\alpha\\). Usando el método de separación de variables obtenemos \\[\\begin{align*}\n    y(t) = \\frac{\\alpha M}{(M - \\alpha) e^{-kt} + \\alpha}.\n\\end{align*}\\] El modelo logístico, con su característico comportamiento sigmoidal es muy utilizado como modelo de crecimiento.\n\n\nEjemplo 1.3 Consideremos incorporar formalmente el efecto de la temperatura en un modelo cinético de segundo orden. Por ejemplo, la hidrólisis del acetato de etilo se modela adecuadamente mediante una cinética de segundo orden. Sea \\(A_{t}\\) la cantidad de acetato de etilo en el tiempo \\(t\\). El modelo de segundo orden es: \\[\n\\frac{dA_{t}}{dt} = -kA_{t}^{2},\n\\] donde \\(k\\) es la constante de velocidad. Las constantes de velocidad dependen de la temperatura, lo cual incorporaremos más adelante en nuestro modelo. Sea \\(A_{0}\\) la cantidad de acetato de etilo en el tiempo cero. La solución a la ecuación de velocidad es \\[\n\\frac{1}{A_{t}} = \\frac{1}{A_{0}} + kt.\n\\] De esta manera, obtenemos \\[\nA_{t} = \\frac{A_{0}}{1 + A_{0}tk}.\n\\] A continuación consideramos el impacto de la temperatura en la constante de velocidad. La ecuación de Arrhenius establece: \\[\nk = C_{1}\\exp\\left(-\\frac{E_{a}}{RT}\\right),\n\\] donde \\(E_{a}\\) es la energía de activación y \\(C_{1}\\) es una constante. Sustituyendo la ecuación de Arrhenius en la ecuación de velocidad obtenemos \\[\nA_{t} = \\frac{A_{0}}{1 + A_{0}tC_{1}\\exp\\left(-E_{a}/RT\\right)}.\n\\] Por lo tanto, un modelo adecuado de regresión no lineal es \\[\nA_{t} = \\frac{\\theta_{1}}{1 + \\theta_{2}t\\exp\\left(-\\theta_{3}/T\\right)} + \\varepsilon_{t},\n\\] donde \\(\\theta_{1}=A_{0}\\), \\(\\theta_{2}=C_{1}A_{0}\\) y \\(\\theta_{3}=E_{a}/R\\).\n\nObservemos que en todos los ejemplos, se presenta una relación no lineal respecto a los parámetros del modelo. Este es el tipo de funciones que se espera encontrar en los modelos de regresión no lineal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regresión no lineal</span>"
    ]
  },
  {
    "objectID": "Caps/01-Introduccion.html#el-modelo-de-regresión-no-lineal",
    "href": "Caps/01-Introduccion.html#el-modelo-de-regresión-no-lineal",
    "title": "1  Regresión no lineal",
    "section": "1.3 El modelo de regresión no lineal",
    "text": "1.3 El modelo de regresión no lineal\nEn general, escribiremos el modelo de regresión no lineal como \\[\n    y = f\\left(\\boldsymbol{x}, \\boldsymbol{\\theta}\\right) + \\varepsilon\n\\] donde \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_p)^\\top\\) es un vector de tamaño \\(p \\times 1\\) de parámetros desconocidos y \\(\\varepsilon\\) es un término de error aleatorio no correlacionado con \\(\\mathbb{E}(\\varepsilon) = 0\\) y \\(\\text{Var}(\\varepsilon) = \\sigma^{2}\\). También suponemos típicamente que los errores están distribuidos normales, como en la regresión lineal. Dado que \\[\\begin{align*}\n    \\mathbb E\\left({y}\\right) &= \\mathbb E\\left[f\\left(\\boldsymbol{x}, \\boldsymbol{\\theta}\\right) + \\varepsilon\\right]\\\\\n&= f\\left(\\boldsymbol{x}, \\boldsymbol{\\theta}\\right)\n\\end{align*}\\] llamamos a \\(f(\\boldsymbol{x}, \\boldsymbol{\\theta})\\) la o () para el modelo de regresión no lineal. Esto es muy similar al caso de regresión lineal, excepto que ahora la función de respuesta esperada es una de los parámetros.\nEn regresión lineal, las derivadas de la función de respuesta esperada no son funciones de los parámetros desconocidos. En un modelo de regresión no lineal, al menos una de las derivadas de la función de respuesta esperada con respecto a los parámetros depende de al menos uno de los parámetros. Para enfatizar la diferencia entre el modelo de regresión lineal y el modelo de regresión no lineal, denotaremos a los parámetros del modelo por \\(\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_p)^\\top\\) para el primero, mientras que para el segundo utilizaremos \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_p)^\\top\\). Ilustremos esto con un ejemplo, consideremos un modelo de regresión lineal: \\[\n    \\boldsymbol y = f\\left(\\boldsymbol{x},\\boldsymbol{\\beta}\\right) + \\boldsymbol \\varepsilon = \\beta_{0} + \\sum_{j=1}^{k}\\beta_{j}x_{j} + \\boldsymbol \\varepsilon.\n\\] Ahora, \\[\n\\frac{\\partial f\\left(\\boldsymbol{x},\\boldsymbol{\\beta}\\right)}{\\partial\\beta_{j}} = x_{j}, \\quad j=0,1, \\ldots, k,\n\\] donde \\(\\boldsymbol x_{0} \\equiv 1\\). Nótese que en el caso lineal las derivadas son constantes con respecto a \\(\\boldsymbol{\\beta}\\).\nPor otro lado, consideremos el modelo no lineal \\[\ny = f\\left(x,\\boldsymbol{\\theta}\\right) + \\varepsilon = \\theta_{1}e^{\\theta_{2}x} + \\varepsilon.\n\\] Las derivadas de la función expectativa con respecto a \\(\\theta_{1}\\) y \\(\\theta_{2}\\) son: \\[\n\\frac{\\partial f\\left(x,\\boldsymbol{\\theta}\\right)}{\\partial\\theta_{1}} = e^{\\theta_{2}x} \\quad \\text{y} \\quad \\frac{\\partial f\\left(x,\\boldsymbol{\\theta}\\right)}{\\partial\\theta_{2}} = \\theta_{1}xe^{\\theta_{2}x}.\n\\] Dado que las derivadas son función de los parámetros desconocidos \\(\\theta_{1}\\) y \\(\\theta_{2}\\), el modelo es no lineal.\n\n\n\n\nBates, Douglas M., y Donald G. Watts. 1988. Nonlinear regression analysis and its applications. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nGaray, Aldo M., Víctor H. Lachos, Filidor V. Labra, y Edwin M. M. Ortega and. 2014. «Statistical diagnostics for nonlinear regression models based on scale mixtures of skew-normal distributions». Journal of Statistical Computation and Simulation 84 (8): 1761-78. https://doi.org/10.1080/00949655.2013.766188.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, y G. Geoffrey Vining. 2012. Introduction to linear regression analysis. 5.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nRitz, Christian, y Jens C. Streibig. 2008. Nonlinear Regression with R. Use R! New York: Springer. https://doi.org/10.1007/978-0-387-09616-2.\n\n\nRuckstuhl, Andreas. 2010. «Introduction to Nonlinear Regression». IDP Institut für Datenanalyse und Prozessdesign, ZHAW Zürcher Hochschule für Angewandte Wissenschaften.\n\n\nSchabenberger, Oliver, y Francis J. Pierce. 2002. Contemporary Statistical Models for the Plant and Soil Sciences. Taylor & Francis, CRC Press.\n\n\nSeber, G. A. F, y Wild C. J. 2003. Nonlinear regression. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regresión no lineal</span>"
    ]
  },
  {
    "objectID": "Caps/02-Definiciones.html",
    "href": "Caps/02-Definiciones.html",
    "title": "2  Métodos de estimación",
    "section": "",
    "text": "2.1 Transformación a modelos lineales\nSe han proporcionado varios ejemplos en los que una relación de tendencia no lineal entre \\(y\\) y \\(x\\) puede transformarse para obtener una relación lineal. Debido a la relativa simplicidad de los métodos de regresión lineal, trabajar con el modelo linealizado es muy atractivo.\nA veces es útil considerar una transformación que induzca linealidad en la función de expectativa del modelo. Por ejemplo, consideremos el modelo \\[\n    y = f(x, \\boldsymbol\\theta) + \\varepsilon = \\theta_1 e^{\\theta_2 x} + \\varepsilon\n\\qquad(2.1)\\] La Ecuación 1.3 de crecimiento poblacional en el Ejemplo 1.1 es un ejemplo de este modelo. Dado que \\(\\mathbb{E}(y) = f(x, \\theta) = \\theta_1 e^{\\theta_2 x}\\), podemos linealizar la función de expectativa tomando logaritmos, \\[\n\\ln \\mathbb{E}(y) = \\ln \\theta_1 + \\theta_2 x.\n\\] Por lo tanto, es tentador considerar reescribir el modelo como \\[\n    \\ln y = \\ln \\theta_1 + \\theta_2 x + \\varepsilon = \\beta_0 + \\beta_1 x + \\varepsilon\n\\qquad(2.2)\\] y usar regresión lineal simple para estimar \\(\\beta_0\\) y \\(\\beta_1\\). Sin embargo, las estimaciones por mínimos cuadrados lineales de los parámetros en la Ecuación 2.2 generalmente no serán equivalentes a las estimaciones no lineales de los parámetros en el modelo original Ecuación 2.1. La razón es que en el modelo no lineal original, los mínimos cuadrados implican la minimización de la suma de residuos al cuadrado en \\(y\\), mientras que en el modelo transformado de la Ecuación 2.2 estamos minimizando la suma de residuos al cuadrado en \\(\\ln y\\).\nAdemás, notemos que en la Ecuación 2.1 la estructura del error es aditiva, por lo que tomar logaritmos no puede producir el modelo en la Ecuación 2.2. Si la estructura del error es multiplicativa, por ejemplo \\[\ny = \\theta_1 e^{\\theta_2 x} \\varepsilon\n\\] entonces tomar logaritmos será apropiado, ya que \\[\n\\ln y = \\ln \\theta_1 + \\theta_2 x + \\ln \\varepsilon = \\beta_0 + \\beta_1 x + \\varepsilon^*\n\\] y si \\(\\varepsilon^*\\) sigue una distribución normal, todas las propiedades estándar del modelo de regresión lineal y la inferencia asociada serán aplicables.\nEl problema a menudo gira en torno a la estructura del error, es decir, ¿se aplican los supuestos estándar sobre los errores al modelo no lineal original o al linealizado? Esta no siempre es una pregunta fácil de responder.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de estimación</span>"
    ]
  },
  {
    "objectID": "Caps/02-Definiciones.html#transformación-a-modelos-lineales",
    "href": "Caps/02-Definiciones.html#transformación-a-modelos-lineales",
    "title": "2  Métodos de estimación",
    "section": "",
    "text": "Ejemplo 2.1 (Modelo de Michaelis–Menten) El modelo de Michaelis–Menten es un modelo de cinética química que relaciona la velocidad inicial de una reacción enzimática con la concentración de sustrato \\(x\\). Dicho modelo es \\[\n    y = \\frac{\\theta_1 x}{x+\\theta_2}+\\varepsilon.\n\\qquad(2.3)\\]\nSe tienen datos de la velocidad inicial de una reacción para una enzima tratada con puromicina, y se desean estimar los coeficientes \\(\\theta_1\\) y \\(\\theta_2\\).\n\n\nCódigo\nlibrary(dplyr)\nlibrary(nortest)\nlibrary(MASS)\nlibrary(fitdistrplus)\n\n\nLos datos son los siguientes:\n\n\nCódigo\ndata(Puromycin)\nhead(Puromycin)\n\n\n  conc rate   state\n1 0.02   76 treated\n2 0.02   47 treated\n3 0.06   97 treated\n4 0.06  107 treated\n5 0.11  123 treated\n6 0.11  139 treated\n\n\nA continuación haremos una gráfica para analizarlos visualmente.\n\n\nCódigo\ndata &lt;- Puromycin %&gt;% filter(state == \"treated\")\n\nplot(data$conc, data$rate, xlab = \"Concentración (ppm)\", ylab = \"Velocidad\", main = \"Velocidad de reacción Puromicina\", pch = 20, col = \"red\")\nlegend(\"bottomright\", legend = c(\"Datos\"), col = c(\"red\"), pch = c(20))\ngrid()\n\n\n\n\n\n\n\n\n\nPodemos notar que la función de respuesta esperada puede ser linealizada fácilmente como sigue: \\[\n\\frac{1}{f(x, \\boldsymbol{\\theta)}} = \\frac{x+\\theta_2}{\\theta_1 x} = \\frac{1}{\\theta_1} + \\frac{\\theta_2}{\\theta_1}x.\n\\]\nPor lo tanto, un primer acercamiento es ajustar el modelo lineal \\[\ny^\\star = \\beta_0 + \\beta_1 u + \\varepsilon,\n\\] en donde \\(y^\\star = 1/y\\) y \\(u = 1/x\\). El modelo lineal ajustado resulta ser:\n\n\nCódigo\nlinearmodel &lt;- lm(I(1 / rate) ~ I(1 / conc), data = data)\nsummary(linearmodel)\n\n\n\nCall:\nlm(formula = I(1/rate) ~ I(1/conc), data = data)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0043103 -0.0003742 -0.0000510  0.0004549  0.0038084 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.0051072  0.0007040   7.255 2.74e-05 ***\nI(1/conc)   0.0002472  0.0000321   7.700 1.64e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001892 on 10 degrees of freedom\nMultiple R-squared:  0.8557,    Adjusted R-squared:  0.8413 \nF-statistic:  59.3 on 1 and 10 DF,  p-value: 1.642e-05\n\n\nA continuación, veremos una gráfica del ajuste de dicho modelo.\n\n\nCódigo\nplot(data$conc, data$rate, xlab = \"Concentración (ppm)\", ylab = \"Velocidad\", main = \"Velocidad de reacción Puromicina\", pch = 20, col = \"red\")\nxslin &lt;- seq(0.001, 1.5, length.out = 1000)\nlines(xslin, 1 / (linearmodel$coefficients[1] + linearmodel$coefficients[2] / (xslin)))\ngrid()\nlegend(\"bottomright\", legend = c(\"Datos\", \"Regresión lineal\"), col = c(\"red\", \"black\"), pch = c(20, NA), lty = c(NA, 1))\n\n\n\n\n\n\n\n\n\nPara analizar el ajuste del modelo, podemos considerar las siguientes gráficas.\n\n\nCódigo\npar(mfrow = c(2, 2), oma = c(0, 0, 0, 0))\nplot(linearmodel)\n\n\n\n\n\n\n\n\n\nComo podemos ver, los errores no parecen ser homocedásticos, y más aún, los residuales no parecen seguir una distribución normal. Para comprobarlo, hacemos la prueba de Anderson - Darling, y obtenemos:\n\n\nCódigo\nad.test(linearmodel$residuals)\n\n\n\n    Anderson-Darling normality test\n\ndata:  linearmodel$residuals\nA = 1.0423, p-value = 0.006107\n\n\nPor lo tanto, este modelo no parece ser útil para explicar nuestros datos.\nConsiderando que \\(\\beta_0 = 1/\\theta_1\\) y \\(\\beta_1 = \\theta_2/\\theta_1\\), los valores de \\(\\boldsymbol{\\theta}\\) ajustados son:\n\n\nCódigo\nunname(c(1 / linearmodel$coefficients[1], linearmodel$coefficients[2] / linearmodel$coefficients[1]))\n\n\n[1] 195.80270885   0.04840653",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de estimación</span>"
    ]
  },
  {
    "objectID": "Caps/02-Definiciones.html#mínimos-cuadrados-no-lineales",
    "href": "Caps/02-Definiciones.html#mínimos-cuadrados-no-lineales",
    "title": "2  Métodos de estimación",
    "section": "2.2 Mínimos cuadrados no lineales",
    "text": "2.2 Mínimos cuadrados no lineales\nSupongamos que tenemos una muestra de \\(n\\) observaciones de la variable de respuesta y los regresores, digamos \\(y_{i}, x_{i1}, x_{i2}, \\ldots, x_{ik}\\), para \\(i=1,2,\\ldots,n\\). Recordemos que el modelo de regresión lineal está dado por \\[\n    \\boldsymbol y = X \\boldsymbol \\beta + \\boldsymbol \\varepsilon,\n\\] donde \\[\n    \\boldsymbol{y} = \\left[\\begin{array}{c}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{array}\\right], \\quad\n\\boldsymbol{X_j}=\\left[\\begin{array}{c}\nx_{1j} \\\\\nx_{2j} \\\\\n\\vdots \\\\\nx_{nj}\n\\end{array}\\right], \\ j=1,\\ldots,k \\quad\n\\boldsymbol{\\beta}=\\left[\\begin{array}{c}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{k}\n\\end{array}\\right], \\quad\n\\boldsymbol{\\varepsilon}=\\left[\\begin{array}{c}\n\\varepsilon_{1} \\\\\n\\varepsilon_{2} \\\\\n\\vdots \\\\\n\\varepsilon_{n}\n\\end{array}\\right] \\sim N(\\boldsymbol 0, \\sigma^2 I_n),\n\\] y \\(X\\) es la matriz con columnas \\[\n    X = [\\boldsymbol 1, \\boldsymbol X_1, \\ldots, \\boldsymbol X_n];\n\\] y el método de mínimos cuadrados en regresión lineal implica minimizar la función de mínimos cuadrados \\[\n    S(\\boldsymbol \\beta) = \\sum_{i = 1}^n \\left(y_i - [X\\boldsymbol \\beta]_i\\right)^2.\n\\]\nDebido a que este es un modelo de regresión lineal, cuando diferenciamos \\(S(\\boldsymbol{\\beta})\\) con respecto a los parámetros desconocidos e igualamos las derivadas a cero, las ecuaciones normales resultantes son ecuaciones lineales, y por lo tanto, son fáciles de resolver.\nAhora consideremos la situación de regresión no lineal. El modelo es \\[\ny_{i} = f\\left( \\boldsymbol{x}_{i}, \\boldsymbol{\\theta} \\right) + \\varepsilon_{i}, \\quad i=1,2,\\ldots,n,\n\\] donde ahora \\(\\boldsymbol{x}_{i} = (x_{i1}, x_{i2}, \\ldots, x_{ik})\\) para \\(i=1,2,\\ldots,n\\). La función de mínimos cuadrados es \\[\nS(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} \\left( y_{i} - f\\left( \\boldsymbol{x}_{i}, \\boldsymbol{\\theta} \\right) \\right)^{2}.\n\\] Para encontrar las estimaciones de mínimos cuadrados debemos diferenciar la ecuación anterior con respecto a cada elemento de \\(\\boldsymbol{\\theta}\\). Esto proporcionará un conjunto de \\(p\\) ecuaciones normales para la situación de regresión no lineal. Las ecuaciones normales son \\[\n\\sum_{i=1}^{n} \\left( y_{i} - f\\left( \\boldsymbol{x}_{i}, \\boldsymbol{\\theta} \\right) \\right) {\\left[ \\frac{\\partial f\\left( \\boldsymbol{x}_{i}, \\boldsymbol{\\theta} \\right)}{\\partial \\theta_{j}} \\right]}_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}} = 0 \\quad \\text{para } j=1,2,\\ldots,p.\n\\] En un modelo de regresión no lineal, las derivadas de la función de respuesta esperada serán funciones de los parámetros desconocidos. Además, la función \\(f\\) también es una función no lineal, por lo que las ecuaciones normales pueden ser muy difíciles de resolver. Al igual que en el modelo de regresión lineal, se espera que el número de datos \\(n\\) sea mayor que el número de parámetros a estimar \\(p\\).\n\nEjemplo 2.2 Consideremos el modelo de regresión no lineal \\[\ny = \\theta_1 e^{\\theta_2 x} + \\varepsilon.\n\\] Las ecuaciones normales de mínimos cuadrados para este modelo son \\[\n\\begin{split}\n    \\sum_{i=1}^{n} \\left( y_i - \\hat{\\theta}_1 e^{\\hat{\\theta}_2 x_i} \\right) e^{\\hat{\\theta}_2 x_i} &= 0,\\\\\n\\sum_{i=1}^{n} \\left( y_i - \\hat{\\theta}_1 e^{\\hat{\\theta}_2 x_i} \\right) \\hat{\\theta}_1 x_i e^{\\hat{\\theta}_2 x_i} &= 0.\n\\end{split}\n\\] Después de simplificar, las ecuaciones normales quedan:\n\\[\n\\begin{split}\n    \\sum_{i=1}^{n} y_i e^{\\hat{\\theta}_2 x_i} - \\hat{\\theta}_1 \\sum_{i=1}^{n} e^{2\\hat{\\theta}_2 x_i} &= 0,\\\\\n    \\sum_{i=1}^{n} y_i x_i e^{\\hat{\\theta}_2 x_i} - \\hat{\\theta}_1 \\sum_{i=1}^{n} x_i e^{2\\hat{\\theta}_2 x_i} &= 0.\n\\end{split}\n\\] Estas ecuaciones no son lineales en \\(\\hat{\\theta}_1\\) y \\(\\hat{\\theta}_2\\), y no existe una solución cerrada simple.\n\nEn general, se deben usar métodos iterativos para encontrar los valores de \\(\\hat\\theta_1\\) y \\(\\hat{\\theta}_2\\). Más aún, a veces existen múltiples soluciones para las ecuaciones normales. Es decir, hay múltiples valores estacionarios para la función de suma de cuadrados residuales \\(S(\\boldsymbol\\theta)\\).\n\n2.2.1 Estimadores de Máxima Verosimilitud\nNos hemos enfocado en mínimos cuadrados para el caso no lineal. Sin embargo, si los términos de error en el modelo están distribuidos de forma normal e independiente con varianza constante, el método de máxima verosimilitud conducirá a mínimos cuadrados.\nEn efecto, en el modelo no lineal \\[\n    y_i = f(\\boldsymbol{x_i}; \\boldsymbol{\\theta}) + \\varepsilon_i, \\qquad i = 1, 2, \\ldots, n,\n\\] si los errores están distribuidos normalmente e independientemente con media cero y varianza \\(\\sigma^2\\), entonces la función de verosimilitud es: \\[\nL(\\boldsymbol\\theta, \\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left[ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left[y_i - f(\\boldsymbol{x}_i; \\boldsymbol{\\theta})\\right]^2 \\right].\n\\]\nClaramente, maximizar esta función de verosimilitud es equivalente a minimizar la suma de cuadrados residuales. Por lo tanto, en el caso de teoría normal, las estimaciones por mínimos cuadrados son iguales a las estimaciones de máxima verosimilitud.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de estimación</span>"
    ]
  },
  {
    "objectID": "Caps/02-Definiciones.html#estimación-de-parámetros-en-modelos-no-lineales",
    "href": "Caps/02-Definiciones.html#estimación-de-parámetros-en-modelos-no-lineales",
    "title": "2  Métodos de estimación",
    "section": "2.3 Estimación de parámetros en modelos no lineales",
    "text": "2.3 Estimación de parámetros en modelos no lineales\n\n2.3.1 Geometría de Mínimos Cuadrados Lineales y No Lineales\nEl estudio de la geometría del problema de mínimos cuadrados resulta útil para comprender las complejidades introducidas por un modelo no lineal. Para una muestra dada, la función de suma de cuadrados de los residuos \\(S(\\boldsymbol{\\theta})\\) depende únicamente de los parámetros del modelo \\(\\boldsymbol{\\theta}\\). Por lo tanto, en el espacio de parámetros (definido por \\(\\theta_1, \\theta_2, \\ldots, \\theta_p\\)), podemos representar la función \\(S(\\boldsymbol{\\theta})\\) mediante un gráfico de contornos, donde cada contorno en la superficie corresponde a una línea de suma de cuadrados de residuos constante.\nSupongamos que el modelo de regresión es lineal; es decir, los parámetros son \\(\\boldsymbol{\\theta} = \\boldsymbol{\\beta}\\), y la función de suma de cuadrados de los residuos es \\(S(\\boldsymbol{\\beta})\\). La Figura 2.1 a) muestra el gráfico de contornos para este caso. Cuando el modelo es lineal en los parámetros desconocidos, los contornos son elipsoidales y presentan un único mínimo global en el estimador de mínimos cuadrados \\(\\hat{\\boldsymbol{\\beta}}\\).\nCuando el modelo es no lineal, los contornos suelen adoptar la forma mostrada en la Figura 2.1 b). Nótese que estos contornos no son elípticos, sino bastante alargados y de forma irregular. Es muy común observar una apariencia en forma de “banana”. La forma y orientación específicas de los contornos de la suma de cuadrados de los residuos dependen tanto de la forma del modelo no lineal como de la muestra de datos obtenida. Con frecuencia, la superficie estará muy alargada cerca del óptimo, por lo que muchas soluciones para \\(\\boldsymbol{\\theta}\\) producirán una suma de cuadrados de residuos cercana al mínimo global. Esto da lugar a un problema mal condicionado, donde suele ser difícil encontrar el mínimo global para \\(\\boldsymbol{\\theta}\\). En algunos casos, los contornos pueden ser tan irregulares que aparecen varios mínimos locales e incluso más de un mínimo global. La Figura 2.1 c) ilustra una situación con un mínimo global y posibles áreas donde podría detenerse el método numérico como posible mínimo.\n\n\n\n\n\n\nFigura 2.1: Geometría\n\n\n\n\n\n2.3.2 Linealización y el método de Gauss-Newton\nUn método ampliamente utilizado en algoritmos computacionales para regresión no lineal es la linealización de la función no lineal seguida de algún método para estimación de parámetros. Uno de los métodos más sencillos es el método de iteración Gauss-Newton. La linealización se logra mediante una expansión en series de Taylor de \\(f(\\boldsymbol x_i, \\boldsymbol\\theta)\\) alrededor del punto \\(\\boldsymbol\\theta_0 = (\\theta_{1}^{0}, \\theta_{2}^{0}, \\ldots, \\theta_{p}^{0})\\), conservando sólo los términos lineales. De aquí se obtiene el modelo aproximado \\[\n    f(\\boldsymbol{x}_i, \\boldsymbol{\\theta}) \\approx f(\\boldsymbol{x}_i, \\boldsymbol{\\theta}_0) + \\sum_{j=1}^p \\left[ \\frac{\\partial f(\\boldsymbol{x}_i, \\boldsymbol{\\theta})}{\\partial \\theta_j} \\right]_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0} (\\theta_j - \\theta_{j}^{0}).\n\\qquad(2.4)\\] Si definimos \\[\n\\begin{split}\n    f_i^0 &= f(\\boldsymbol{x}_i, \\boldsymbol{\\theta}_0),\\\\\n    \\beta_j^0 &= \\theta_j - \\theta_{j}^{0},\\\\\n    X_{ij}^0 &= \\left[ \\frac{\\partial f(\\boldsymbol{x}_i, \\boldsymbol{\\theta})}{\\partial \\theta_j} \\right]_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0},\n\\end{split}\n\\] observamos que el modelo de regresión no lineal puede escribirse como: \\[\n    y_i - f_i^0 = \\sum_{j=1}^{p} \\beta_j^0 X_{ij}^0 + \\varepsilon_i, \\quad i = 1, 2, \\ldots, n.\n\\qquad(2.5)\\] Es decir, ahora tenemos un modelo de regresión lineal. Normalmente llamamos a \\(\\boldsymbol{\\theta}_{0}\\) los valores iniciales para los parámetros.\nPodemos escribir la Ecuación 2.5 como \\[\n\\boldsymbol{y}_{0} = {X}_0 \\boldsymbol{\\beta}_{0} + \\boldsymbol{\\varepsilon},\n\\] por lo que la estimación de \\(\\boldsymbol{\\beta}_{0}\\) es \\[\n    \\boldsymbol{\\hat{\\beta}}_0 = ({X}_0^\\top {X}_0)^{-1} {X}_0^\\top \\boldsymbol{y}_0 = ({X}_0^\\top {X}_0)^{-1} {X}_0^\\top (\\boldsymbol{y} - \\boldsymbol{f}_0)\n\\qquad(2.6)\\] Dado que \\(\\boldsymbol{\\beta}^{0} = \\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{0}\\), podemos definir \\[\n\\boldsymbol{\\hat{\\theta}}_1 = \\boldsymbol{\\hat{\\beta}}_0 + \\boldsymbol{\\theta}_0\n\\] como las nuevas estimaciones de \\(\\boldsymbol{\\theta}\\). A \\(\\boldsymbol{\\hat{\\beta}}_0\\) también se le conoce como vector de incrementos. Ahora podemos usar las nuevas estimaciones \\(\\boldsymbol{\\hat{\\theta}}_1\\) en la Ecuación 2.4 (en el mismo papel que jugaban las estimaciones iniciales \\(\\boldsymbol{\\theta}_0\\)) para producir otro conjunto de estimaciones, digamos \\(\\boldsymbol{\\hat{\\theta}}_2\\), y así sucesivamente.\nEn general, en la \\(k\\)-ésima iteración tenemos: \\[\n    \\boldsymbol{\\hat{\\theta}}_{k+1} = \\boldsymbol{\\hat{\\theta}}_k + \\boldsymbol{\\hat{\\beta}}_k = \\boldsymbol{\\hat{\\theta}}_k + ({X}_k^\\top {X}_k)^{-1} {X}_k^\\top (\\boldsymbol{y} - \\boldsymbol{f}_k),\n\\qquad(2.7)\\] donde \\[\n\\begin{split}\n    {X}_k &= [X_{ij}^k],\\\\\n    \\boldsymbol{f}_k &= {[f_1^k, f_2^k, \\ldots, f_n^k]}^\\top,\\\\\n    \\boldsymbol{\\hat{\\theta}}_k &= {[\\theta_{1}^k, \\theta_{2}^k, \\ldots, \\theta_{p}^k]}^\\top.\n\\end{split}\n\\] Este proceso iterativo continúa hasta alcanzar la convergencia, es decir, hasta que \\[\n\\frac{\\hat{\\theta}_{j}^{k+1} - \\hat{\\theta}_{j}^{k}}{\\hat{\\theta}_{j}^{k}} &lt; \\delta, \\quad j = 1, 2, \\ldots, p,\n\\] donde \\(\\delta\\) es un número pequeño, por ejemplo \\(1.0 \\times 10^{-6}\\). En cada iteración se debe evaluar la suma de cuadrados residual \\(S(\\boldsymbol{\\hat{\\theta}}_k)\\) para asegurar que se ha obtenido una reducción en su valor.\n\nEjemplo 2.3 Podemos usar el método Gauss-Newton para ajustar el modelo de Michaelis-Menten (Ejemplo 2.1) a los datos de puromicina anteriores, usando los valores iniciales \\(\\theta_{10} = 205\\) y \\(\\theta_{20} = 0.08\\). Más adelante discutiremos cómo se obtuvieron estos valores iniciales. En este punto inicial, la suma de cuadrados residual \\(S(\\boldsymbol{\\theta}_0) = 3155\\). Para ilustrar cómo se calculan las cantidades requeridas, observemos que: \\[\n\\frac{\\partial f(x, \\theta_1, \\theta_2)}{\\partial \\theta_1} = \\frac{x}{\\theta_2 + x} \\quad \\text{y} \\quad \\frac{\\partial f(x, \\theta_1, \\theta_2)}{\\partial \\theta_2} = \\frac{-\\theta_1 x}{\\left(\\theta_2 + x\\right)^2}\n\\] Como la primera observación de \\(x\\) es \\(x_1 = 0.02\\), tenemos: \\[\nX_{11}^0 = \\left. \\frac{x_1}{\\theta_2 + x_1} \\right|_{\\theta_2 = 0.08} = \\frac{0.02}{0.08 + 0.02} = 0.2,\n\\] y también, \\[\nX_{12}^0 = \\left. \\frac{-\\theta_1 x_1}{\\left(\\theta_2 + x_1\\right)^2} \\right|_{\\substack{\\theta_1 = 205 \\\\ \\theta_2 = 0.08}} = \\frac{(-205)(0.02)}{\\left(0.08 + 0.02\\right)^2} = -410.\n\\] Las derivadas \\(X_{ij}^0\\) se recopilan en la matriz \\(\\boldsymbol{X}_0\\) y el vector de incrementos se calcula a partir de la Ecuación 2.6 como: \\[\n\\hat{\\boldsymbol{\\beta}}_0 = \\begin{bmatrix}\n8.03 \\\\\n-0.017\n\\end{bmatrix}.\n\\] La nueva estimación de \\(\\hat{\\boldsymbol{\\theta}}_1\\) a partir de la Ecuación 2.7 es: \\[\n\\hat{\\boldsymbol{\\theta}}_1 = \\hat{\\boldsymbol{\\beta}}_0 + \\boldsymbol{\\theta}_0\n=\n\\begin{bmatrix}\n8.03 \\\\\n-0.017\n\\end{bmatrix} +\n\\begin{bmatrix}\n205.00 \\\\\n0.08\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n213.03 \\\\\n0.063\n\\end{bmatrix}.\n\\] La suma de cuadrados residual en este punto es \\(S(\\hat{\\boldsymbol{\\theta}}_1) = 1206\\), que es considerablemente menor que \\(S(\\boldsymbol{\\theta}_0)\\). Por lo tanto, \\(\\hat{\\boldsymbol{\\theta}}_1\\) se adopta como la nueva estimación de \\(\\boldsymbol{\\theta}\\), y se realizaría otra iteración.\nEl algoritmo Gauss-Newton converge en \\(\\hat{\\boldsymbol{\\theta}}^\\top = [212.7,\\, 0.0641]^\\top\\) con \\(S(\\hat{\\boldsymbol{\\theta}}) = 1195\\). Así, el modelo ajustado obtenido por linealización es: \\[\n\\hat{y} = \\frac{\\hat{\\theta}_1 x}{x + \\hat{\\theta}_2} = \\frac{212.7 x}{x + 0.0641}\n\\] Estos cálculos pueden realizarse automáticamente con la función nls() de R, como sigue.\n\n\nCódigo\nnlmodel &lt;- nls(rate ~ Vm * conc / (K + conc), data = data, start = list(Vm = 205, K = 0.08))\nsummary(nlmodel)\n\n\n\nFormula: rate ~ Vm * conc/(K + conc)\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nVm 2.127e+02  6.947e+00  30.615 3.24e-11 ***\nK  6.412e-02  8.281e-03   7.743 1.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.93 on 10 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 4.157e-06\n\n\nVeamos la gráfica del modelo ajustado en este caso comparada con la obtenida por regresión lineal.\n\n\nCódigo\nplot(data$conc, data$rate, xlab = \"Concentración (ppm)\", ylab = \"Velocidad\", main = \"Velocidad de reacción Puromicina\", pch = 20, col = \"red\")\nxslin &lt;- seq(0.001, 1.5, length.out = 1000)\nlines(xslin, 1 / (linearmodel$coefficients[1] + linearmodel$coefficients[2] / (xslin)))\nlines(xslin, coef(nlmodel)[1] * xslin / (xslin + coef(nlmodel)[2]), col = \"blue\")\ngrid()\nlegend(\"bottomright\", legend = c(\"Datos\", \"Regresión lineal\", \"Regresión no lineal\"), col = c(\"red\", \"black\", \"blue\"), pch = c(20, NA, NA), lty = c(NA, 1, 1))\n\n\n\n\n\n\n\n\n\nComo podemos ver, los datos se explican de mejor manera por medio de la regresión no lineal. También podemos hacer un análisis gráfico del ajuste como a continuación.\n\n\nCódigo\npar(mfrow = c(1, 2), oma = c(0, 0, 0, 0))\n\n# Gráfica de residuales\n\nplot(fitted(nlmodel), residuals(nlmodel), main = \"Residuals vs Fitted\", xlab = \"Fitted values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# PP-plot\nresiduals &lt;- residuals(nlmodel)\nn &lt;- length(residuals)\n\nempirical_probs &lt;- ppoints(n)\nempirical_quantiles &lt;- sort(residuals)\n\ntheoretical_probs &lt;- pnorm(empirical_quantiles,\n  mean = mean(residuals),\n  sd = sd(residuals)\n)\n\nplot(theoretical_probs, empirical_probs,\n  main = \"P-P Plot of Residuals\",\n  xlab = \"Theoretical Normal Probabilities\",\n  ylab = \"Empirical Probabilities\",\n  xlim = c(0, 1), ylim = c(0, 1)\n)\nabline(0, 1, col = \"red\") # Reference line (y = x)\n\n\n\n\n\n\n\n\n\nComo podemos ver, en este caso parecen cumplirse los supuestos de normalidad y homocedasticidad de los errores. Podemos comprobarlo también con la prueba de Anderson - Darling como sigue:\n\n\nCódigo\nad.test(residuals)\n\n\n\n    Anderson-Darling normality test\n\ndata:  residuals\nA = 0.31888, p-value = 0.4885\n\n\nComo vemos, bajo esta prueba, no podemos rechazar la hipótesis de normalidad, y no parece violarse la de homocedasticidad, así que este modelo es más apropiado que el obtenido en regresión lineal.\n\n\n\n2.3.3 Estimación de de la varianza\nCuando el procedimiento de estimación converge a un vector final de estimaciones de parámetros \\(\\hat{\\theta}\\), podemos obtener una estimación de la varianza del error \\(\\sigma^2\\) mediante el error cuadrático medio \\[\n\\hat{\\sigma}^2 = MS_{\\text{Res}} = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-p} = \\frac{\\sum_{i=1}^{n}\\left[y_i - f(\\boldsymbol{x}_i, \\boldsymbol{\\hat{\\theta}})\\right]^2}{n-p} = \\frac{S(\\boldsymbol{\\hat{\\theta}})}{n-p},\n\\] en donde \\(p\\) es el número de parámetros en el modelo de regresión no lineal. También podemos estimar la matriz de covarianzas asintótica (para muestras grandes) del vector de parámetros \\(\\boldsymbol{\\hat{\\theta}}\\) como \\[\n    \\operatorname{Var}(\\boldsymbol{\\hat{\\theta}})=\\hat\\sigma^2 {\\left(\\hat{X}^\\top \\hat{X}\\right)}^{-1},\n\\qquad(2.8)\\] en donde \\(\\hat{X}\\) es la matriz de derivadas parciales definida previamente, evaluada en la última iteración del estimador de mínimos cuadrados \\(\\boldsymbol{\\hat{\\theta}}\\).\n\nEjemplo 2.4 Para los datos de Puromicina, encontramos que la suma de cuadrados residual en la iteración final fue \\(S(\\hat{\\boldsymbol{\\theta}}) = 1195\\), por lo que la estimación de \\(\\sigma^2\\) es: \\[\n\\hat{\\sigma}^2 = \\frac{S(\\hat{\\boldsymbol{\\theta}})}{n - p} = \\frac{1195}{12 - 2} = 119.5.\n\\] También podemos estimar la matriz de covarianza asintótica mediante la Ecuación 2.8 como: \\[\n\\operatorname{Var}(\\boldsymbol{\\hat{\\theta}})=\\sigma^2 {\\left(\\hat{X}^\\top \\hat{X}\\right)}^{-1} = 119.5 \\begin{bmatrix}\n0.4037 & 36.82 \\times 10^{-5} \\\\\n36.82 \\times 10^{-5} & 57.36 \\times 10^{-8}\n\\end{bmatrix}\n\\] Los elementos de la diagonal principal de esta matriz son las varianzas aproximadas de las estimaciones de los coeficientes de regresión. Por lo tanto, los errores estándar aproximados de los coeficientes son: \\[\n\\text{se}(\\hat{\\theta}_1) = \\sqrt{\\text{Var}(\\hat{\\theta}_1)} = \\sqrt{119.5 (0.4037)} = 6.95\n\\] y \\[\n\\text{se}(\\hat{\\theta}_2) = \\sqrt{\\text{Var}(\\hat{\\theta}_2)} = \\sqrt{119.5 (57.36 \\times 10^{-8})} = 8.28 \\times 10^{-3},\n\\] y la correlación entre \\(\\hat{\\theta}_1\\) y \\(\\hat{\\theta}_2\\) es aproximadamente: \\[\n\\frac{36.82 \\times 10^{-5}}{\\sqrt{0.4037 (57.36 \\times 10^{-8})}} = 0.77.\n\\]\n\n\n\n2.3.4 Perspectiva gráfica de linealización\nHemos observado que la función de suma de cuadrados de los residuos \\(S(\\boldsymbol{\\theta})\\) para un modelo de regresión no lineal suele ser una función irregular con forma de “plátano”, como se muestra en las graficas b) y c) de la Figura 2.1 ( P)or otro lado, la función de suma de cuadrados de los residuos para los mínimos cuadrados lineales tiene un comportamiento estable (Figura 2.1 a)). La técnica de linealización convierte el problema de regresión no lineal en una secuencia de problemas lineales, comenzando en el punto \\(\\boldsymbol{\\theta}_0\\).\nLa primera iteración de la linealización reemplaza los contornos irregulares con un conjunto de contornos elípticos. Los contornos irregulares de \\(S(\\boldsymbol{\\theta})\\) pasan exactamente por el punto de partida. El punto \\(\\boldsymbol{\\theta}_0\\), como se muestra en la Figura 2.2 a), es donde comienza el proceso. Al resolver el problema linealizado, nos movemos hacia el mínimo global en el conjunto de contornos elípticos. Esto se logra mediante mínimos cuadrados lineales ordinarios. Luego, la siguiente iteración repite el proceso, comenzando desde la nueva solución \\(\\hat{\\boldsymbol{\\theta}}_1\\). La evolución final de la linealización es una secuencia de problemas lineales cuyas soluciones se “acerca” al mínimo global de la función no lineal, como se ilustra en la Figura 2.2 b). Siempre que el problema no lineal no esté demasiado mal condicionado (ya sea por un modelo mal especificado o datos insuficientes), el procedimiento de linealización debería converger a una buena estimación del mínimo global en pocas iteraciones.\nLa linealización se facilita con un buen valor inicial \\(\\boldsymbol{\\theta}_0\\), es decir, uno que esté razonablemente cerca del mínimo global. Por ende, cuando \\(\\boldsymbol{\\theta}_0\\) está cerca de \\(\\hat{\\boldsymbol{\\theta}}\\), los contornos reales de la suma de cuadrados de los residuos del problema no lineal suelen aproximarse bien mediante los contornos del problema linealizado.\nSin embargo, al tratarse de aproximaciones, es de esperar que si el problema está mal condicionado. Por lo que escoger un buen valor inicial es de suma importancia. Más adelante retomamos este tema, dónde discutimos las ideas básicas sobre cómo escoger buenos valores iniciales.\n\n\n\n\n\n\nFigura 2.2: Linealización\n\n\n\n\n\n2.3.5 Otros métodos de estimación de parámetros\nEl método de linealización básico descrito anteriormente puede converger muy lentamente en algunos problemas. En otros casos, puede generar un movimiento en la dirección equivocada, con la función de suma de cuadrados residuales \\(S(\\hat{\\boldsymbol{\\theta}}_k)\\) aumentando en realidad en la \\(k\\)-ésima iteración. En casos extremos, puede fallar completamente en converger. En consecuencia, se han desarrollado otras técnicas para resolver el problema de regresión no lineal. Algunas de ellas son modificaciones y refinamientos del esquema de linealización. En esta sección presentamos una breve descripción de algunos de estos procedimientos.\nMétodo de Newton-Raphson\nOtro método ampliamente utilizado en la estimación de parámetros en modelos de regresión no lineal es el método iterativo de Newton-Raphson, el cual puede verse como una extensión del método de Gauss-Newton. Este método se basa en una aproximación cuadrática a la función objetivo, en este caso \\(S(\\boldsymbol{\\theta})\\). Supongamos que \\(\\boldsymbol{\\theta}_0\\) es un valor inicial cercano al valor óptimo. Entonces, podemos aproximar \\(S(\\boldsymbol{\\theta})\\) mediante una expansión de Taylor de segundo orden alrededor de \\(\\boldsymbol{\\theta}_0\\): \\[\nS(\\boldsymbol{\\theta}) \\approx S(\\boldsymbol{\\theta}_0) + (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)^\\top \\nabla S(\\boldsymbol{\\theta}_0) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)^\\top H_S(\\boldsymbol{\\theta}_0) (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0),\n\\] donde:\n\n\\(\\nabla S(\\boldsymbol{\\theta}_0)\\) es el gradiente de \\(S\\) evaluado en \\(\\boldsymbol{\\theta}_0\\), y\n\\(H_S(\\boldsymbol{\\theta}_0)\\) es el Hessiano de \\(S\\) evaluado en \\(\\boldsymbol{\\theta}_0\\).\n\nEl mínimo local de \\(S(\\boldsymbol{\\theta})\\) (en caso de que \\(H_S(\\boldsymbol{\\theta}_0)\\) sea definida positiva) ocurre cuando \\[\n\\nabla S(\\boldsymbol{\\theta}_0) + H_S(\\boldsymbol{\\theta}_0)(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) = \\boldsymbol{0},\n\\] lo que lleva a la siguiente regla de actualización: \\[\n\\boldsymbol{\\theta}_1 = \\boldsymbol{\\theta}_0 - H_S(\\boldsymbol{\\theta}_0)^{-1} \\nabla S(\\boldsymbol{\\theta}_0).\n\\] De forma general, el procedimiento iterativo se expresa como: \\[\n\\boldsymbol{\\hat{\\theta}}_{k+1} = \\boldsymbol{\\hat{\\theta}}_k - H_S(\\boldsymbol{\\hat{\\theta}}_k)^{-1} \\nabla S(\\boldsymbol{\\hat{\\theta}}_k).\n\\]\nEn el contexto de regresión no lineal, escribimos la función de pérdida como \\[\nS(\\boldsymbol{\\theta}) = \\|\\boldsymbol{r}(\\boldsymbol{\\theta})\\|^2, \\quad \\text{donde} \\quad \\boldsymbol{r}(\\boldsymbol{\\theta}) = \\boldsymbol{y} - \\boldsymbol{f}(\\boldsymbol{\\theta}),\n\\] por lo que, aplicando la regla de la cadena: \\[\n\\nabla S(\\boldsymbol{\\theta}) = -2 J_f(\\boldsymbol{\\theta})^\\top \\boldsymbol{r}(\\boldsymbol{\\theta}),\n\\] y \\[\nH_S(\\boldsymbol{\\theta}) = 2 J_f(\\boldsymbol{\\theta})^\\top J_f(\\boldsymbol{\\theta}) - 2 \\sum_{i=1}^n r_i(\\boldsymbol{\\theta}) \\, H_{f_i}(\\boldsymbol{\\theta}),\n\\] donde:\n\n\\(J_f(\\boldsymbol{\\theta})\\) es el jacobiano de \\(\\boldsymbol{f}\\) respecto a \\(\\boldsymbol{\\theta}\\), y\n\\(H_{f_i}(\\boldsymbol{\\theta})\\) es el hessiano de la componente \\(f_i\\) respecto a \\(\\boldsymbol{\\theta}\\).\n\nEn la práctica, frecuentemente se descarta el segundo término del Hessiano (que involucra las segundas derivadas de \\(\\boldsymbol{f}\\)), obteniendo así el método de Gauss-Newton como una aproximación al método de Newton-Raphson. Esta omisión se justifica porque \\(J_f^\\top J_f\\) es simétrica y semidefinida positiva, lo que garantiza que la dirección de búsqueda sea de descenso, mientras que el término que involucra los Hessianos de \\(f_i\\) puede introducir inestabilidades.\nMétodo del Descenso por Gradiente\nEl método del descenso por gradiente intenta encontrar el mínimo global de la función de suma de cuadrados residuales mediante minimización directa. El objetivo es moverse desde un punto inicial \\(\\boldsymbol{\\theta}_0\\) en una dirección vectorial con componentes dadas por las derivadas de la función de suma de cuadrados residuales con respecto a los elementos de \\(\\boldsymbol{\\theta}\\), es decir, en dirección \\(-\\nabla S(\\boldsymbol \\theta)\\). Usualmente estas derivadas se estiman ajustando una aproximación de primer orden o planar alrededor del punto \\(\\boldsymbol{\\theta}_0\\). Los coeficientes de regresión en el modelo de primer orden se toman como aproximaciones a las primeras derivadas.\nLa principal desventaja de este método para resolver el problema de regresión no lineal es que puede converger muy lentamente. El descenso por gradiente generalmente funciona mejor cuando el punto inicial está muy alejado del óptimo. Sin embargo, a medida que la solución actual se acerca al óptimo, el procedimiento producirá movimientos cada vez más cortos y un comportamiento “en zig-zag”. Este es el problema de convergencia mencionado anteriormente.\nIncrementos fraccionarios\nUna modificación estándar a la técnica de linealización es el uso de incrementos fraccionarios. Para describir este método, sea \\(\\hat{\\boldsymbol{\\beta}}_{k}\\) el vector de incremento estándar en la Ecuación 2.7 en la \\(k\\)-ésima iteración, pero en este caso continuamos a la siguiente iteración solo si \\(S\\left(\\hat{\\boldsymbol{\\theta}}_{k+1}\\right) &lt; S\\left(\\hat{\\boldsymbol{\\theta}}_{k}\\right)\\). Si \\(S\\left(\\hat{\\boldsymbol{\\theta}}_{k+1}\\right) &gt; S\\left(\\hat{\\boldsymbol{\\theta}}_{k}\\right)\\), se usa \\(\\hat{\\boldsymbol{\\beta}}_{k}/2\\) como vector de incrementos. Esta división podría usarse varias veces durante una iteración, si es necesario.\nSi después de un número específico de intentos no se obtiene una reducción en \\(S\\left(\\hat{\\boldsymbol{\\theta}}_{k+1}\\right)\\), el procedimiento se termina. La idea general detrás de este método es evitar que el procedimiento de linealización realice un paso demasiado grande en cualquier iteración. La técnica de incrementos fraccionarios es útil cuando se encuentran problemas de convergencia en el procedimiento de linealización básico.\nMétodo de Marquardt\nOtra modificación popular al algoritmo básico de linealización fue desarrollada por Marquardt en 1963. Él propuso calcular el vector de incrementos en la \\(k\\)-ésima iteración mediante: \\[\n    \\left({X}_{k}^{\\top}{X}_{k} + \\lambda\\boldsymbol{I}_{p}\\right)\\hat{\\boldsymbol{\\beta}}_{k} = {X}_{k}^{\\top}\\left(\\boldsymbol{y} - \\boldsymbol{f}_{k}\\right)\n\\qquad(2.9)\\] donde \\(\\lambda &gt; 0\\). Notemos la similitud con el estimador de regresión Ridge estudiado en el curso. Dado que las variables regresoras son derivadas de la misma función, la función linealizada podría generar problemas de multicolinealidad. Por lo tanto, el procedimiento tipo Ridge en la Ecuación 2.9 es intuitivamente razonable. Marquardt usó un procedimiento de búsqueda para encontrar un valor de \\(\\lambda\\) que redujera la suma de cuadrados residual en cada etapa.\nDiferentes programas computacionales seleccionan \\(\\lambda\\) de distintas formas, por ejemplo, algunos comienzan con \\(\\lambda = 10^{-8}\\). Se realizan una serie de cálculos de prueba y error en cada iteración con \\(\\lambda\\) multiplicado repetidamente por \\(10\\) hasta que: \\[\nS\\left(\\hat{\\boldsymbol{\\theta}}_{k+1}\\right) &lt; S\\left(\\hat{\\boldsymbol{\\theta}}_{k}\\right)\n\\] El procedimiento también implica reducir \\(\\lambda\\) por un factor de \\(10\\) en cada iteración mientras se satisfaga la desigualdad anterior. La estrategia es mantener \\(\\lambda\\) tan pequeño como sea posible mientras se asegura que la suma de cuadrados residual se reduzca en cada iteración. Este procedimiento general se denomina a menudo método de Marquardt (Marquardt’ s Compromise), porque el vector resultante de incrementos producido por su método generalmente se encuentra entre el vector Gauss-Newton en el vector de linealización y la dirección del descenso más pronunciado.\nA continuación se muestra una comparación entre los distintos métodos de estimación discutidos. Para ello, se considera un modelo de la forma \\[\n    y = e^{\\theta x} + \\varepsilon,\n\\] donde \\(\\varepsilon \\sim N(0,0.05)\\). Para poder realizar la comparación, simulamos datos siguiendo el modelo anterior con un parámetro verdadero de \\(\\theta = -1.5\\) y comparamos la convergencia de los distintos métodos graficando los resultados de cada una de las iteraciones.\n\n\nCódigo\nset.seed(42)\n\n# Datos simulados\nn &lt;- 100\nx &lt;- seq(0, 2, length.out = n)\ntrue_theta &lt;- -1.5\ny &lt;- exp(true_theta * x) + rnorm(n, sd = 0.05)\n\n# Función de suma de mínimos cuadrados\nrss &lt;- function(theta) {\n  sum((y - exp(theta * x))^2)\n}\n\n# Gradiente de suma de mínimos cuadrados\ngrad_rss &lt;- function(theta) {\n  -2 * sum(x * exp(theta * x) * (y - exp(theta * x)))\n}\n\n# Hesiano de suma de mínimos cuadrados\nhess_rss &lt;- function(theta) {\n  2 * sum((x^2) * exp(2 * theta * x) - x^2 * y * exp(theta * x))\n}\n\n# 1. Gauss-Newton\ngauss_newton &lt;- function(theta0, tol = 1e-6, max_iter = 100) {\n  theta &lt;- theta0\n  traza &lt;- theta\n  for (i in 1:max_iter) {\n    f &lt;- exp(theta * x)\n    r &lt;- y - f\n    J &lt;- -x * f\n    H_approx &lt;- sum(J^2)\n    g &lt;- sum(J * r)\n    delta &lt;- g / H_approx\n    theta &lt;- theta + delta\n    traza &lt;- c(traza, theta)\n    if (abs(delta) &lt; tol) break\n  }\n  traza\n}\n\n# 2. Newton-Raphson\nnewton_raphson &lt;- function(theta0, tol = 1e-6, max_iter = 100) {\n  theta &lt;- theta0\n  traza &lt;- theta\n  for (i in 1:max_iter) {\n    g &lt;- grad_rss(theta)\n    H &lt;- hess_rss(theta)\n    delta &lt;- -g / H\n    theta &lt;- theta + delta\n    traza &lt;- c(traza, theta)\n    if (abs(delta) &lt; tol) break\n  }\n  traza\n}\n\n# 3. Levenberg-Marquardt\nmarquardt &lt;- function(theta0, lambda = 0.01, tol = 1e-6, max_iter = 100) {\n  theta &lt;- theta0\n  traza &lt;- theta\n  for (i in 1:max_iter) {\n    g &lt;- grad_rss(theta)\n    H &lt;- hess_rss(theta)\n    H_lm &lt;- H + lambda * abs(H) # Damping\n    delta &lt;- -g / H_lm\n    theta_new &lt;- theta + delta\n    if (rss(theta_new) &lt; rss(theta)) {\n      theta &lt;- theta_new\n      lambda &lt;- lambda / 2\n    } else {\n      lambda &lt;- lambda * 2\n    }\n    traza &lt;- c(traza, theta)\n    if (abs(delta) &lt; tol) break\n  }\n  traza\n}\n\n# 4. Descenso del gradiente\nsteepest_descent &lt;- function(theta0, alpha = 0.02, tol = 1e-6, max_iter = 100) {\n  theta &lt;- theta0\n  traza &lt;- theta\n  for (i in 1:max_iter) {\n    grad &lt;- grad_rss(theta)\n    theta_new &lt;- theta - alpha * grad\n    traza &lt;- c(traza, theta_new)\n    if (abs(theta_new - theta) &lt; tol) break\n    theta &lt;- theta_new\n  }\n  traza\n}\n\n# 5. Incrementos fracionarios\nfractional_increments &lt;- function(theta0, alpha = 0.01, delta = 1e-4, tol = 1e-6, max_iter = 100) {\n  theta &lt;- theta0\n  traza &lt;- theta\n  for (i in 1:max_iter) {\n    f0 &lt;- rss(theta)\n    f1 &lt;- rss(theta + delta)\n    slope &lt;- (f1 - f0) / delta\n    theta_new &lt;- theta - alpha * slope\n    traza &lt;- c(traza, theta_new)\n    if (abs(theta_new - theta) &lt; tol) break\n    theta &lt;- theta_new\n  }\n  traza\n}\n\n# Valor inicial\ntheta0 &lt;- -0.5\n\n# Correr todos los métodos\ntrazas &lt;- list(\n  \"Gauss-Newton\" = gauss_newton(theta0),\n  \"Newton-Raphson\" = newton_raphson(theta0),\n  \"Levenberg-Marquardt\" = marquardt(theta0),\n  \"Descenso del Gradiente\" = steepest_descent(theta0),\n  \"Incrementos Fractionarios\" = fractional_increments(theta0)\n)\n\n# Gráfica\ncolores &lt;- c(\"red\", \"blue\", \"darkgreen\", \"purple\", \"orange\", \"black\")\nplot(NA,\n  xlim = c(0, 30), ylim = c(-2, 0),\n  xlab = \"Iteración\", ylab = expression(theta),\n  main = \"Convergencia de los Métodos de Optimización\"\n)\n\nlegend(\"topright\", legend = c(names(trazas), \"Verdadero Parámetro\"), col = colores, lty = c(rep(1, 5), 2), bty = \"n\")\n\nfor (i in seq_along(trazas)) {\n  lines(0:(length(trazas[[i]]) - 1), trazas[[i]], type = \"l\", col = colores[i])\n}\n\nabline(h = true_theta, col = \"black\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n2.3.6 Valores iniciales\nAjustar un modelo de regresión no lineal requiere valores iniciales \\(\\boldsymbol{\\theta}_0\\) para los parámetros del modelo. Buenos valores iniciales, es decir, valores de \\(\\boldsymbol{\\theta}_0\\) cercanos a los valores verdaderos de los parámetros, minimizarán las dificultades de convergencia. Las modificaciones al procedimiento de linealización como el método de Marquardt han hecho que el procedimiento sea menos sensible a la elección de valores iniciales, pero siempre es una buena idea seleccionar \\(\\boldsymbol{\\theta}_0\\) cuidadosamente. Una mala elección podría causar convergencia a un mínimo local en la función, y podríamos estar completamente inconscientes de que se ha obtenido una solución subóptima.\nEn los modelos de regresión no lineal, los parámetros a menudo tienen algún significado físico, y esto puede ser muy útil para obtener valores iniciales. También puede ser útil graficar la función expectativa para varios valores de los parámetros para familiarizarse con el comportamiento del modelo y cómo los cambios en los valores de los parámetros afectan este comportamiento.\nPor ejemplo, en la función de Michaelis-Menten utilizada para los datos de puromicina (Ejemplo 2.1), el parámetro \\(\\theta_1\\) es la velocidad asintótica de la reacción, es decir, el valor máximo de \\(f\\) cuando \\(x \\to \\infty\\). De manera similar, \\(\\theta_2\\) representa la concentración media, o el valor de \\(x\\) tal que cuando la concentración alcanza ese valor, la velocidad es la mitad del valor máximo. Al obervar las gráficas de los datos, se podrían sugerir como valores iniciales razonables \\(\\theta_1 = 205\\) y \\(\\theta_2 = 0.08\\), que fueron los utilizados anteriormente.\nEn algunos casos podemos transformar la función expectativa para obtener valores iniciales. Por ejemplo, el modelo de Michaelis-Menten puede “linealizarse” tomando el recíproco de la función expectativa. Se pueden usar mínimos cuadrados lineales en los datos recíprocos, lo que resulta en estimaciones de los parámetros lineales. Estas estimaciones pueden usarse luego para obtener los valores iniciales necesarios \\(\\boldsymbol{\\theta}_0\\). La transformación gráfica también puede ser muy efectiva.\n\n\n\n\nBates, Douglas M., y Donald G. Watts. 1988. Nonlinear regression analysis and its applications. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nGaray, Aldo M., Víctor H. Lachos, Filidor V. Labra, y Edwin M. M. Ortega and. 2014. «Statistical diagnostics for nonlinear regression models based on scale mixtures of skew-normal distributions». Journal of Statistical Computation and Simulation 84 (8): 1761-78. https://doi.org/10.1080/00949655.2013.766188.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, y G. Geoffrey Vining. 2012. Introduction to linear regression analysis. 5.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nRitz, Christian, y Jens C. Streibig. 2008. Nonlinear Regression with R. Use R! New York: Springer. https://doi.org/10.1007/978-0-387-09616-2.\n\n\nRuckstuhl, Andreas. 2010. «Introduction to Nonlinear Regression». IDP Institut für Datenanalyse und Prozessdesign, ZHAW Zürcher Hochschule für Angewandte Wissenschaften.\n\n\nSchabenberger, Oliver, y Francis J. Pierce. 2002. Contemporary Statistical Models for the Plant and Soil Sciences. Taylor & Francis, CRC Press.\n\n\nSeber, G. A. F, y Wild C. J. 2003. Nonlinear regression. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Métodos de estimación</span>"
    ]
  },
  {
    "objectID": "Caps/03-Inferencia.html",
    "href": "Caps/03-Inferencia.html",
    "title": "3  Inferencia estadística en regresión no lineal",
    "section": "",
    "text": "3.1 Intervalos y Regiones de confianza\nEn un modelo de regresión lineal, cuando los errores están distribuidos de forma normal e independiente, existen pruebas estadísticas exactas e intervalos de confianza basados en las distribuciones \\(t\\) y \\(F\\), y las estimaciones de los parámetros tienen propiedades estadísticas útiles y atractivas. Sin embargo, este no es el caso en la regresión no lineal, incluso cuando los errores están distribuidos normalmente e independientemente.\nEs decir, en la regresión no lineal, las estimaciones por mínimos cuadrados (o máxima verosimilitud) de los parámetros del modelo no poseen las propiedades atractivas que tienen sus contrapartes en la regresión lineal, como insesgamiento, varianza mínima o distribuciones muestrales normales. La inferencia estadística en regresión no lineal depende de resultados asintóticos. La teoría de muestras grandes generalmente se aplica tanto para errores distribuidos normalmente como no normales.\nLos resultados asintóticos clave pueden resumirse brevemente de la siguiente manera. En general, cuando el tamaño de la muestra \\(n\\) es grande, el valor esperado de \\(\\boldsymbol{\\hat{\\theta}}\\) es aproximadamente igual a \\(\\boldsymbol{\\theta}\\), el verdadero vector de estimaciones de parámetros, y la matriz de covarianza de \\(\\boldsymbol{\\hat{\\theta}}\\) es aproximadamente \\(\\sigma^2(\\hat{X}^\\top \\hat{X})^{-1}\\), donde \\(\\hat{X}\\) es la matriz de derivadas parciales evaluadas en la estimación de mínimos cuadrados de la última iteración \\(\\boldsymbol{\\hat{\\theta}}\\). Además, la distribución muestral de \\(\\boldsymbol{\\hat{\\theta}}\\) es aproximadamente normal.\nEn consecuencia, la inferencia estadística para regresión no lineal cuando el tamaño de muestra es grande se realiza exactamente igual que para regresión lineal. Las pruebas estadísticas e intervalos de confianza son solo procedimientos aproximados.\nAnteriormente comentamos que siguiendo las ideas de regresión lineal, podemos obtener aproximaciones para las regiones de confianza y los intervalos de confianza en el caso de regresión no lineal, cuando el tamaño de la muestra es grande. De esta manera, tenemos que los intervalos de confianza para \\(\\boldsymbol\\theta\\) están dados por \\[\n    \\widehat{\\theta}_j \\pm t(\\alpha / 2; n-p) \\sqrt{ MS_{\\operatorname{Res}} C_{j j}}.\n\\] donde \\(\\hat{X}\\) es la matriz de derivadas parciales definida previamente, evaluada en la última iteración del estimador de mínimos cuadrados \\(\\boldsymbol{\\hat{\\theta}}\\), \\(C_{jj}\\) es la \\(j\\)-ésima entrada en la diagonal de la matriz \\(C = (\\hat{X}^\\top\\hat{X})^{-1}\\), \\[\n    \\text{ECM} = \\frac{S(\\hat{\\boldsymbol\\theta})}{n-p}\n\\] es el error cuadrado medio, y \\(t(n-p)\\) es la distribución t de Student de \\(n-p\\) grados de libertad.\nDe igual manera, las regiones de inferencia aproximada para un modelo no lineal se definen como \\[\n    (\\theta - \\hat{\\theta})^{\\top} \\hat X^{\\top} \\hat X (\\theta - \\hat{\\theta}) \\leq p \\text{ECM} F(p, n - p; \\alpha)\n\\qquad(3.1)\\] en donde \\(F(p, n - p; \\alpha)\\) es el cuantil superior \\(\\alpha\\) para la distribución F de Fisher con $p $ y \\(n - p\\) grados de libertad.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia estadística en regresión no lineal</span>"
    ]
  },
  {
    "objectID": "Caps/03-Inferencia.html#intervalos-y-regiones-de-confianza",
    "href": "Caps/03-Inferencia.html#intervalos-y-regiones-de-confianza",
    "title": "3  Inferencia estadística en regresión no lineal",
    "section": "",
    "text": "Ejemplo 3.1 Reconsideremos el modelo de Michaelis-Menten para los datos de puromicina analizados previamente. Para probar la significancia de la regresión (es decir, \\(H_{0}\\): \\(\\theta_{1}=\\theta_{2}=0\\)) podríamos usar un procedimiento similar a un ANOVA. Podemos calcular la suma total de cuadrados de las \\(y\\) como \\(SS_{T}=271\\ 909.0\\). Entonces, la suma de cuadrados del modelo o regresión es: \\[\nSS_{\\text{modelo}} = SS_{T} - SS_{\\text{Res}} = 271\\ 410 - 1\\ 195.4 = 270\\ 214.6.\n\\] Por lo tanto, la prueba de significancia de la regresión es: \\[\nF_{0} = \\frac{SS_{\\text{modelo}}/2}{MS_{\\text{Res}}} = \\frac{270\\ 214.6/2}{1\\ 195.4/(12-2)} = 1\\ 130.22,\n\\] y calculamos un valor \\(P\\) aproximado a partir de la distribución \\(F_{2,10}\\). Este valor \\(P\\) es considerablemente menor que \\(0.0001\\), por lo que podemos rechazar con seguridad la hipótesis nula y concluir que al menos uno de los parámetros del modelo es distinto de cero.\nPara probar hipótesis sobre los parámetros individuales del modelo, \\(H_{0}\\): \\(\\theta_{1}=0\\) y \\(H_{0}\\): \\(\\theta_{2}=0\\), podríamos calcular estadísticos \\(t\\) aproximados como: \\[\nt_{1} = \\frac{\\hat{\\theta}_{1}}{\\text{se}\\left(\\hat{\\theta}_{1}\\right)} = \\frac{212.7}{6.9471} = 30.62\n\\] y \\[\nt_{2} = \\frac{\\hat{\\theta}_{2}}{\\text{se}\\left(\\hat{\\theta}_{2}\\right)} = \\frac{0.0641}{0.00828} = 7.74.\n\\] Los valores \\(P\\) aproximados para estos dos estadísticos de prueba son ambos menores que \\(0.01\\). Por lo tanto, podríamos concluir que ambos parámetros son distintos de cero con una confianza del \\(99\\%\\).\nLos intervalos de confianza aproximados del \\(95\\%\\) de confianza para \\(\\theta_{1}\\) se encuentran de la siguiente manera: \\[\n\\begin{array}{rcl}\n    \\hat{\\theta}_{1} - t_{0.025,10}\\,\\text{se}\\left(\\hat{\\theta}_{1}\\right) &\\leq \\theta_{1} &\\leq \\hat{\\theta}_{1} + t_{0.025,10}\\,\\text{se}\\left(\\hat{\\theta}_{1}\\right)\\\\\n    212.7 - 2.228\\,(6.9471) &\\leq \\theta_{1} &\\leq 212.7 + 2.228\\,(6.9471)\\\\\n    197.2 &\\leq \\theta_{1} &\\leq 228.2.\n\\end{array}\n\\] Análogamente, para \\(\\theta_2\\): \\[\n\\begin{array}{rcl}\n    t_{0.025,10}\\,\\text{se}\\left(\\hat{\\theta}_{2}\\right) &\\leq \\theta_{2} &\\leq \\hat{\\theta}_{2} + t_{0.025,10}\\,\\text{se}\\left(\\hat{\\theta}_{2}\\right)\\\\\n    0.0641 - 2.228\\,(0.00828) &\\leq \\theta_{2} &\\leq 0.0641 + 2.228\\,(0.00828)\\\\\n    0.0457 &\\leq \\theta_{2} &\\leq 0.0825.\n\\end{array}\n\\] En lo anterior también se puede ver que ninguno de los intervalos contiene al \\(0\\), lo que nos permite asegurar que ambos parámetros son distintos de cero con una confianza del \\(95\\%\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia estadística en regresión no lineal</span>"
    ]
  },
  {
    "objectID": "Caps/03-Inferencia.html#validez-de-la-inferencia-aproximada",
    "href": "Caps/03-Inferencia.html#validez-de-la-inferencia-aproximada",
    "title": "3  Inferencia estadística en regresión no lineal",
    "section": "3.2 Validez de la inferencia aproximada",
    "text": "3.2 Validez de la inferencia aproximada\nDado que las pruebas, procedimientos e intervalos de confianza en regresión no lineal se basan en teoría de grandes muestras y típicamente el tamaño de muestra en un problema de regresión no lineal puede no ser tan grande, es lógico cuestionar la validez de estos procedimientos. Sería deseable tener una guía o “regla general” que nos indique cuándo el tamaño de muestra es lo suficientemente grande para que los resultados asintóticos sean válidos. Desafortunadamente, no existe tal guía general. Sin embargo, existen algunos indicadores que sugieren que los resultados pueden ser válidos en una aplicación particular.\n\nSi el algoritmo de estimación de regresión no lineal converge en pocas iteraciones, esto indica que la aproximación lineal utilizada para resolver el problema fue muy satisfactoria, y es probable que los resultados asintóticos se apliquen adecuadamente. Una convergencia que requiere muchas iteraciones es un síntoma de que los resultados asintóticos podrían no aplicarse, y deberían considerarse otras verificaciones de adecuación.\nSe han desarrollado varias medidas de curvatura y no linealidad del modelo. Estas medidas describen cuantitativamente la adecuación de la aproximación lineal. Nuevamente, una aproximación lineal inadecuada indicaría que los resultados de inferencia asintótica son cuestionables.\nSe puede usar bootstrap para estudiar la distribución muestral de los estimadores, calcular errores estándar aproximados y encontrar intervalos de confianza aproximados. Podríamos calcular estimaciones bootstrap de estas cantidades y compararlas con los errores estándar e intervalos de confianza aproximados producidos por los resultados asintóticos. Una buena concordancia con las estimaciones bootstrap es un indicio de que los resultados de inferencia de grandes muestras son válidos.\n\nCuando hay indicios de que los resultados de inferencia asintótica no son válidos, tenemos pocas opciones a realizar. Una posibilidad es considerar una forma alternativa del modelo, si existe, o quizás un modelo de regresión no lineal diferente. A veces, gráficos de los datos y gráficos de diferentes funciones de respuesta esperada (\\(f\\)) de modelos no lineales pueden ser útiles en este sentido. Alternativamente, se pueden usar los resultados de inferencia de remuestreo o bootstrap. Sin embargo, si el modelo es incorrecto o está mal especificado, hay pocas razones para creer que los resultados de remuestreo serán más válidos que los resultados de inferencia de grandes muestras.\n\n\n\n\nBates, Douglas M., y Donald G. Watts. 1988. Nonlinear regression analysis and its applications. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nGaray, Aldo M., Víctor H. Lachos, Filidor V. Labra, y Edwin M. M. Ortega and. 2014. «Statistical diagnostics for nonlinear regression models based on scale mixtures of skew-normal distributions». Journal of Statistical Computation and Simulation 84 (8): 1761-78. https://doi.org/10.1080/00949655.2013.766188.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, y G. Geoffrey Vining. 2012. Introduction to linear regression analysis. 5.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nRitz, Christian, y Jens C. Streibig. 2008. Nonlinear Regression with R. Use R! New York: Springer. https://doi.org/10.1007/978-0-387-09616-2.\n\n\nRuckstuhl, Andreas. 2010. «Introduction to Nonlinear Regression». IDP Institut für Datenanalyse und Prozessdesign, ZHAW Zürcher Hochschule für Angewandte Wissenschaften.\n\n\nSchabenberger, Oliver, y Francis J. Pierce. 2002. Contemporary Statistical Models for the Plant and Soil Sciences. Taylor & Francis, CRC Press.\n\n\nSeber, G. A. F, y Wild C. J. 2003. Nonlinear regression. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inferencia estadística en regresión no lineal</span>"
    ]
  },
  {
    "objectID": "Caps/04-Validacion.html",
    "href": "Caps/04-Validacion.html",
    "title": "4  Validación del modelo",
    "section": "",
    "text": "4.1 Medidas de ajuste",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Validación del modelo</span>"
    ]
  },
  {
    "objectID": "Caps/04-Validacion.html#medidas-de-ajuste",
    "href": "Caps/04-Validacion.html#medidas-de-ajuste",
    "title": "4  Validación del modelo",
    "section": "",
    "text": "4.1.1 Análisis de residuos y observaciones influyentes\nEn el caso de regresión no lineal, tenemos las mismas hipótesis sobre los residuos que en el caso de regresión lineal. Así que, al igual que antes, podemos hacer un análisis de residuos análogo al caso de regresión lineal. Para ello, definimos los residuales como \\[\n    \\hat{e}_i = y_i - f(\\boldsymbol{x_i}, \\hat{\\boldsymbol{\\theta}}), \\qquad i = 1, \\ldots, n.\n\\] De esta manera, se pueden considerar las siguientes gráficas como antes:\n\ngráfico de residuos contra valores ajustados (gráfica de los pares \\((f(\\boldsymbol{x_i}, \\hat{\\boldsymbol{\\theta}}), \\hat{e}_i)\\)),\ngráfico de escala-ubicación de \\(\\sqrt{\\left|\\hat{e}_i\\right|}\\) contra valores ajustados.\n\nAlternativamente podemos considerar los errores normalizados \\[\n    \\hat{r}_i = \\frac{\\hat{e_i}}{\\sqrt{\\hat{s}^2}}.\n\\] Si las hipótesis del modelo son satisfechas, los residuos normalizados deberían estar distribuidos aproximadamente como una normal estándar. Para valorar esto, podemos considerar\n\ngráfico Q-Q normal,\nprueba de normalidad como la de Anderson-Darling o de Shapiro-Wilk.\n\nPara verificar la independencia entre los errores, podemos considerar\n\ngráfico de retraso de los residuos contra los residuos anteriores,\n\ndonde si se observa alguna relación lineal entre los puntos, podría indicar dependencia entre los errores.\nFinalmente, para verificar si existen observaciones influyentes, se puede definir la distancia de Cook generalizada dada por \\[\n    GD_i = \\ell_{(i)}(\\hat{\\boldsymbol{\\theta}})^\\top \\mathbf{J}(\\hat{\\boldsymbol{\\theta}})^{-1} \\ell_{(i)}(\\hat{\\boldsymbol{\\theta}}),\n\\] donde\n\n\\(\\ell_{(i)}(\\hat{\\boldsymbol{\\theta}}) = -\\partial \\ell_i(\\boldsymbol{\\theta}) / \\partial \\boldsymbol{\\theta} \\big|_{\\hat{\\boldsymbol{\\theta}}}\\) es el gradiente de la log-verosimilitud sin la observación \\(i\\), evaluado en \\(\\hat{\\boldsymbol{\\theta}}\\);\n\\(\\mathbf{J}(\\hat{\\boldsymbol{\\theta}})\\) es la matriz de información observada de Fisher, que cuantifica la curvatura del modelo en \\(\\hat{\\boldsymbol{\\theta}}\\);\n\ny podemos considerar la gráfica\n\ngráfico de distancias de Cook contra las etiquetas de las filas.\n\nComo podemos observar, el tipo de gráficos que se utilizan son similares a los utilizados en regresión lineal. Sin embargo, dado que los modelos no lineales pierden ciertas propiedades deseables, debemos ajustar ciertas definiciones.\n\n\n4.1.2 Pseudo-\\(R^2\\)\nAl igual que en el caso de regresión lineal, podemos definir una “\\(R^2\\)” para los modelos de regresión no lineal, dada por \\[\n    R^2 = 1 - \\frac{SS_{Res}}{SS_T},\n\\] donde \\[\n    SS_{Res} = \\sum_{i = 1}^{n}{\\left(y_i - \\hat{y_i} \\right)^2}, \\qquad SS_{T} = \\sum_{i = 1}^{n}{\\left(y_i - \\bar{y} \\right)^2}.\n\\] Sin embargo, en este caso esta ya no cuenta con las propiedades con las que contaba en los modelos de regresión lineal, pues:\n\nno puede ser superior a 1, pero sí inferior a 0;\nno puede interpretarse como la proporción de varianza explicada por el modelo.\n\nEsta es la razón por la que en modelos de regresión no lineal se le suele llamar pseudo-\\(R^2\\) en vez de sólo \\(R^2\\).\nA pesar de esto, esta estadística sigue siendo de utilidad para revisar si el ajuste de un modelo es bueno. Si la pseudo-\\(R^2\\) es cercana a 1, quiere decir que el ajuste ha sido muy bueno. En comparación, si el ajuste es cercano a 0 o incluso negativo, esto implica que hay problemas con el ajuste del modelo.\nSobre todo, recordemos que la pseudo-\\(R^2\\), de forma similar a la \\(R^2\\) en la regresión lineal múltiple, nunca debe utilizarse como base para seleccionar y comparar modelos de regresión alternativos. Para ello deben utilizarse otros estadísticos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Validación del modelo</span>"
    ]
  },
  {
    "objectID": "Caps/04-Validacion.html#comparación-de-modelos",
    "href": "Caps/04-Validacion.html#comparación-de-modelos",
    "title": "4  Validación del modelo",
    "section": "4.2 Comparación de modelos",
    "text": "4.2 Comparación de modelos\nEn algunas situaciones, puede haber más de una función que podría utilizarse como modelo. Por ejemplo, al ajustar un modelo de doble exponencial, \\[\n    f(x,\\theta) = \\theta_1 e^{-\\theta_2 x} + \\theta_3 e^{-\\theta_4 x},\n\\] \\(\\theta_4\\) podría ser 0, en cuyo caso el modelo se reduce a \\[\n    f(x,\\theta) = \\theta_1 e^{-\\theta_2 x} + \\theta_3,\n\\] o \\(\\theta_3\\) podría ser 0, reduciéndose a \\[\n    f(x,\\theta) = \\theta_1 e^{-\\theta_2 x}.\n\\]\nEn esta situación de modelos anidados, estaríamos interesados en encontrar el modelo más simple que se ajuste adecuadamente a los datos.\nEn otros casos, podríamos comparar modelos no anidados. Por ejemplo, el Modelo 1: \\[\n    f(x,\\theta) = \\theta_1 (1 - e^{-\\theta_2 x}),\n\\] contra el Modelo 2: \\[\n    f(x,\\theta) = \\frac{\\theta_1 x}{\\theta_2 + x}.\n\\]\nAmbos comienzan en $ f = 0 $ cuando $ x = 0 $ y se aproximan a la asíntota \\(\\theta_1\\) cuando $ x $. En estos casos, un modelo puede proporcionar un ajuste superior a los datos, y nos interesaría seleccionar dicho modelo.\n\n4.2.1 Modelos anidados\nCuando comparamos varios modelos anidados, procedemos como en el caso lineal y utilizamos una prueba de razón de verosimilitud para decidir cuál es el modelo más simple que se ajusta adecuadamente a un conjunto de datos. Debido al supuesto de normalidad esférica, esto conduce a una evaluación de la suma de cuadrados adicional debida a los parámetros extra involucrados al pasar del modelo parcial al modelo completo.\nDenotemos por \\(S\\) a la suma de cuadrados, \\(\\mathbf{v}\\) los grados de libertad y \\(p\\) el número de parámetros, con subíndices \\(T\\) y \\(R\\) para los modelos completo y no completo, y un subíndice \\(E\\) para extra, los cálculos se pueden resumir como en el cuadro Tabla 4.1. Para completar el análisis, comparamos el cociente \\(s_{E}^{2}/s_{T}^{2}\\) con \\(F(\\mathbf{v}_{E},\\mathbf{v}_{T};\\mathbf{\\alpha})\\) y aceptamos el modelo parcial si el cociente de cuadrados medios calculado es menor que el valor de la tabla. De lo contrario, retenemos los términos adicionales y usamos el modelo completo.\n\n\n\nTabla 4.1: Análisis de suma de cuadrados adicional para modelos anidados.\n\n\n\n\n\n\n\n\n\n\n\n\nFuente\nSuma de Cuadrados\nGrados de Libertad\nCuadrado Medio\nRazón F\n\n\n\n\nParámetros extra\n\\(S_{E} = S_{R}-S_{T}\\)\n\\(\\mathbf{v}_{E}=p_{T}-p_{R}\\)\n\\(s_{E}^{2}=S_{E}/\\mathbf{v}_{E}\\)\n\\(s_{E}^{2}/s_{T}^{2}\\)\n\n\nModelo completo\n\\(S_{T}\\)\n\\(\\mathbf{v}_{T}=n-p_{T}\\)\n\\(s_{T}^{2}=S_{T}/\\mathbf{v}_{T}\\)\n\n\n\nModelo parcial\n\\(S_{R}\\)\n\\(n-p_{R}\\)\n\n\n\n\n\n\n\n\nPara modelos no lineales, en comparación a modelos lineales, el análisis es solo aproximado porque el cociente de cuadrados medios calculado no tendrá una distribución F exacta. Sin embargo, la distribución del cociente de cuadrados medios solo se ve afectada por la no linealidad intrínseca y no por la no linealidad de efectos de parámetros, además que la no linealidad intrínseca es generalmente pequeña. Cuando el modelo parcial es inadecuado, el efecto de la no linealidad intrínseca en el análisis puede ser grande, pero el modelo parcial se rechazará de todos modos: es solo cuando los valores ajustados están muy cercanos que la forma de la distribución es crítica. En estos casos, la no linealidad intrínseca generalmente tendrá un efecto pequeño porque las respuestas esperadas que se comparan están cercanas en la superficie de expectativa.\n\n\n4.2.2 Modelos no anidados\nAl intentar decidir cuál de varios modelos no anidados es el mejor, el primer enfoque debe corresponder al investigador. Es decir, si existen razones científicas para preferir un modelo sobre los demás, se debe dar un peso considerable a las razones del investigador, ya que el objetivo principal del análisis de datos es explicar o dar cuenta del comportamiento de los datos, no simplemente obtener el mejor ajuste.\nSi el investigador no puede proporcionar razones convincentes para elegir un modelo sobre otros, entonces se pueden utilizar análisis estadísticos, siendo probablemente el más importante un análisis de los residuos. Generalmente, se debe elegir el modelo con el cuadrado medio residual más pequeño y los residuos de apariencia más aleatoria. Los residuos deben graficarse contra los valores predichos, las variables de control, el orden temporal y cualquier otra variable.\n\n\n\n\nBates, Douglas M., y Donald G. Watts. 1988. Nonlinear regression analysis and its applications. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nGaray, Aldo M., Víctor H. Lachos, Filidor V. Labra, y Edwin M. M. Ortega and. 2014. «Statistical diagnostics for nonlinear regression models based on scale mixtures of skew-normal distributions». Journal of Statistical Computation and Simulation 84 (8): 1761-78. https://doi.org/10.1080/00949655.2013.766188.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, y G. Geoffrey Vining. 2012. Introduction to linear regression analysis. 5.ª ed. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: Wiley.\n\n\nRitz, Christian, y Jens C. Streibig. 2008. Nonlinear Regression with R. Use R! New York: Springer. https://doi.org/10.1007/978-0-387-09616-2.\n\n\nRuckstuhl, Andreas. 2010. «Introduction to Nonlinear Regression». IDP Institut für Datenanalyse und Prozessdesign, ZHAW Zürcher Hochschule für Angewandte Wissenschaften.\n\n\nSchabenberger, Oliver, y Francis J. Pierce. 2002. Contemporary Statistical Models for the Plant and Soil Sciences. Taylor & Francis, CRC Press.\n\n\nSeber, G. A. F, y Wild C. J. 2003. Nonlinear regression. Wiley series en probability y mathematical statistics. Applied probability y statistics. New York: John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Validación del modelo</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Bates, Douglas M., and Donald G. Watts. 1988. Nonlinear Regression\nAnalysis and Its Applications. Wiley Series in Probability and\nMathematical Statistics. Applied Probability and Statistics. New York:\nWiley.\n\n\nGaray, Aldo M., Víctor H. Lachos, Filidor V. Labra, and Edwin M. M.\nOrtega and. 2014. “Statistical Diagnostics for Nonlinear\nRegression Models Based on Scale Mixtures of Skew-Normal\nDistributions.” Journal of Statistical Computation and\nSimulation 84 (8): 1761–78. https://doi.org/10.1080/00949655.2013.766188.\n\n\nMontgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2012.\nIntroduction to Linear Regression Analysis. 5th ed. Wiley\nSeries in Probability and Mathematical Statistics. Applied Probability\nand Statistics. New York: Wiley.\n\n\nRitz, Christian, and Jens C. Streibig. 2008. Nonlinear Regression\nwith R. Use R! New York: Springer. https://doi.org/10.1007/978-0-387-09616-2.\n\n\nRuckstuhl, Andreas. 2010. “Introduction to Nonlinear\nRegression.” IDP Institut für Datenanalyse und Prozessdesign,\nZHAW Zürcher Hochschule für Angewandte Wissenschaften.\n\n\nSchabenberger, Oliver, and Francis J. Pierce. 2002. Contemporary\nStatistical Models for the Plant and Soil Sciences. Taylor &\nFrancis, CRC Press.\n\n\nSeber, G. A. F, and Wild C. J. 2003. Nonlinear Regression.\nWiley Series in Probability and Mathematical Statistics. Applied\nProbability and Statistics. New York: John Wiley & Sons.",
    "crumbs": [
      "Referencias"
    ]
  }
]