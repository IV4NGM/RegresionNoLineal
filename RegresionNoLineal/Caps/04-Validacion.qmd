# Validación del modelo

## Medidas de ajuste

### Análisis de residuos y observaciones influyentes

En el caso de regresión no lineal, tenemos las mismas hipótesis sobre los residuos que en el caso de regresión lineal. Así que, al igual que antes, podemos hacer un análisis de residuos análogo al caso de regresión lineal. Para ello, definimos los residuales como
$$
    \hat{e}_i = y_i - f(\boldsymbol{x_i}, \hat{\boldsymbol{\theta}}), \qquad i = 1, \ldots, n.
$$
De esta manera, se pueden considerar las siguientes gráficas como antes:

1. gráfico de residuos contra valores ajustados (gráfica de los pares $(f(\boldsymbol{x_i}, \hat{\boldsymbol{\theta}}), \hat{e}_i)$),
2. gráfico de escala-ubicación de $\sqrt{\left|\hat{e}_i\right|}$ contra valores ajustados.

Alternativamente podemos considerar los errores normalizados
$$
    \hat{r}_i = \frac{\hat{e_i}}{\sqrt{\hat{s}^2}}.
$$
Si las hipótesis del modelo son satisfechas, los residuos normalizados deberían estar distribuidos aproximadamente como una normal estándar. Para valorar esto, podemos considerar

3. gráfico Q-Q normal,
4. prueba de normalidad como la de Anderson-Darling o de Shapiro-Wilk.

Para verificar la independencia entre los errores, podemos considerar

5. gráfico de retraso de los residuos contra los residuos anteriores,

donde si se observa alguna relación lineal entre los puntos, podría indicar dependencia entre los errores.

Finalmente, para verificar si existen observaciones influyentes, se puede definir la distancia de Cook generalizada dada por
$$
    GD_i = \ell_{(i)}(\hat{\boldsymbol{\theta}})^\top \mathbf{J}(\hat{\boldsymbol{\theta}})^{-1} \ell_{(i)}(\hat{\boldsymbol{\theta}}),
$$
donde

* $\ell_{(i)}(\hat{\boldsymbol{\theta}}) = -\partial \ell_i(\boldsymbol{\theta}) / \partial \boldsymbol{\theta} \big|_{\hat{\boldsymbol{\theta}}}$ es el gradiente de la log-verosimilitud sin la observación $i$, evaluado en $\hat{\boldsymbol{\theta}}$;
* $\mathbf{J}(\hat{\boldsymbol{\theta}})$ es la matriz de información observada de Fisher, que cuantifica la curvatura del modelo en $\hat{\boldsymbol{\theta}}$;

y podemos considerar la gráfica

6. gráfico de distancias de Cook contra las etiquetas de las filas.

Como podemos observar, el tipo de gráficos que se utilizan son similares a los utilizados en regresión lineal. Sin embargo, dado que los modelos no lineales pierden ciertas propiedades deseables, debemos ajustar ciertas definiciones.

### Pseudo-$R^2$

Al igual que en el caso de regresión lineal, podemos definir una "$R^2$" para los modelos de regresión no lineal, dada por
$$
    R^2 = 1 - \frac{SS_{Res}}{SS_T},
$$
donde
$$
    SS_{Res} = \sum_{i = 1}^{n}{\left(y_i - \hat{y_i} \right)^2}, \qquad SS_{T} = \sum_{i = 1}^{n}{\left(y_i - \bar{y} \right)^2}.
$$
Sin embargo, en este caso esta ya no cuenta con las propiedades con las que contaba en los modelos de regresión lineal, pues:

* no puede ser superior a 1, pero sí inferior a 0;
* no puede interpretarse como la proporción de varianza explicada por el modelo.

Esta es la razón por la que en modelos de regresión no lineal se le suele llamar pseudo-$R^2$ en vez de sólo $R^2$.

A pesar de esto, esta estadística sigue siendo de utilidad para revisar si el ajuste de un modelo es bueno. Si la pseudo-$R^2$ es cercana a 1, quiere decir que el ajuste ha sido muy bueno. En comparación, si el ajuste es cercano a 0 o incluso negativo, esto implica que hay problemas con el ajuste del modelo.

Sobre todo, recordemos que la pseudo-$R^2$, de forma similar a la $R^2$ en la regresión lineal múltiple, nunca debe utilizarse como base para seleccionar y comparar modelos de regresión alternativos. Para ello deben utilizarse otros estadísticos.

## Comparación de modelos

En algunas situaciones, puede haber más de una función que podría utilizarse como modelo. Por ejemplo, al ajustar un modelo de doble exponencial,
$$
    f(x,\theta) = \theta_1 e^{-\theta_2 x} + \theta_3 e^{-\theta_4 x},
$$
$\theta_4$ podría ser 0, en cuyo caso el modelo se reduce a
$$
    f(x,\theta) = \theta_1 e^{-\theta_2 x} + \theta_3,
$$
o $\theta_3$ podría ser 0, reduciéndose a
$$
    f(x,\theta) = \theta_1 e^{-\theta_2 x}.
$$

En esta situación de modelos anidados, estaríamos interesados en encontrar el modelo más simple que se ajuste adecuadamente a los datos.

En otros casos, podríamos comparar modelos no anidados. Por ejemplo, el Modelo 1:
$$
    f(x,\theta) = \theta_1 (1 - e^{-\theta_2 x}),
$$
contra el Modelo 2:
$$
    f(x,\theta) = \frac{\theta_1 x}{\theta_2 + x}.
$$

Ambos comienzan en $ f = 0 $ cuando $ x = 0 $ y se aproximan a la asíntota $\theta_1$ cuando $ x \to \infty $. En estos casos, un modelo puede proporcionar un ajuste superior a los datos, y nos interesaría seleccionar dicho modelo.

### Modelos anidados

Cuando comparamos varios modelos anidados, procedemos como en el caso lineal y utilizamos una prueba de razón de verosimilitud para decidir cuál es el modelo más simple que se ajusta adecuadamente a un conjunto de datos. Debido al supuesto de normalidad esférica, esto conduce a una evaluación de la suma de cuadrados adicional debida a los parámetros extra involucrados al pasar del modelo parcial al modelo completo.

Denotemos por $S$ a la suma de cuadrados, $\mathbf{v}$ los grados de libertad y $p$ el número de parámetros, con subíndices $T$ y $R$ para los modelos completo y no completo, y un subíndice $E$ para extra, los cálculos se pueden resumir como en el cuadro @tbl-comparacion-modelos. Para completar el análisis, comparamos el cociente $s_{E}^{2}/s_{T}^{2}$ con $F(\mathbf{v}_{E},\mathbf{v}_{T};\mathbf{\alpha})$ y aceptamos el modelo parcial si el cociente de cuadrados medios calculado es menor que el valor de la tabla. De lo contrario, retenemos los términos adicionales y usamos el modelo completo.


| Fuente           | Suma de Cuadrados | Grados de Libertad | Cuadrado Medio | Razón F            |
|------------------|-------------------|--------------------|----------------|--------------------|
| Parámetros extra | $S_{E} = S_{R}-S_{T}$ | $\mathbf{v}_{E}=p_{T}-p_{R}$ | $s_{E}^{2}=S_{E}/\mathbf{v}_{E}$ | $s_{E}^{2}/s_{T}^{2}$ |
| Modelo completo  | $S_{T}$           | $\mathbf{v}_{T}=n-p_{T}$ | $s_{T}^{2}=S_{T}/\mathbf{v}_{T}$ |                    |
| Modelo parcial   | $S_{R}$           | $n-p_{R}$          |                |                    |

: Análisis de suma de cuadrados adicional para modelos anidados. {#tbl-comparacion-modelos}

Para modelos no lineales, en comparación a modelos lineales, el análisis es solo aproximado porque el cociente de cuadrados medios calculado no tendrá una distribución F exacta. Sin embargo, la distribución del cociente de cuadrados medios solo se ve afectada por la no linealidad intrínseca y no por la no linealidad de efectos de parámetros, además que la no linealidad intrínseca es generalmente pequeña. Cuando el modelo parcial es inadecuado, el efecto de la no linealidad intrínseca en el análisis puede ser grande, pero el modelo parcial se rechazará de todos modos: es solo cuando los valores ajustados están muy cercanos que la forma de la distribución es crítica. En estos casos, la no linealidad intrínseca generalmente tendrá un efecto pequeño porque las respuestas esperadas que se comparan están cercanas en la superficie de expectativa.

### Modelos no anidados

Al intentar decidir cuál de varios modelos no anidados es el mejor, el primer enfoque debe corresponder al investigador. Es decir, si existen razones científicas para preferir un modelo sobre los demás, se debe dar un peso considerable a las razones del investigador, ya que el objetivo principal del análisis de datos es explicar o dar cuenta del comportamiento de los datos, no simplemente obtener el mejor ajuste.

Si el investigador no puede proporcionar razones convincentes para elegir un modelo sobre otros, entonces se pueden utilizar análisis estadísticos, siendo probablemente el más importante un análisis de los residuos. Generalmente, se debe elegir el modelo con el cuadrado medio residual más pequeño y los residuos de apariencia más aleatoria. Los residuos deben graficarse contra los valores predichos, las variables de control, el orden temporal y cualquier otra variable.